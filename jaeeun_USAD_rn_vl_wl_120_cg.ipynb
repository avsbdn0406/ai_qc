{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3983f-a7ad-4eb9-83c9-1c34cb9b9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6f2bf-d6f3-4836-be76-562ee67f5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True) # input dim (3) -> hidden dim (LSTM)\n",
    "        self.fc = nn.Linear(hidden_dim, latent_dim) # hidden dim -> latent dim (z) (FC layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        h, _ = self.lstm(x)  # h: [B, T, H]\n",
    "        h_last = h[:, -1, :]  # Last hidden state (sequence summary)\n",
    "        z = self.fc(h_last)  # [B, latent_dim]\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002e835-e8d7-4757-b323-8f49f227e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, seq_len=10, num_layers=1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim) # latent dim -> hidden dim (FC layer)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True) #hidden dim -> hidden dim (LSTM)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # hidden dim -> output dim (3) (FC layer)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, latent_dim]\n",
    "        h0 = self.latent_to_hidden(z)  # latent vector [B, H] to hidden state\n",
    "        h0_seq = h0.unsqueeze(1).repeat(1, self.seq_len, 1)  # Copy for the sequence length (T) [B, T, H]\n",
    "        h_seq, _ = self.lstm(h0_seq) # Inference for 1 step (Make sequence)\n",
    "        out = self.fc(h_seq)  # Recover for 3dim var with each time step output [B, T, output_dim]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834838c9-402b-4911-a51a-ba47afa2bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USAD_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, seq_len=10):\n",
    "        super(USAD_LSTM, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder1 = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_len)\n",
    "        self.decoder2 = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        return w1, w2, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac2cc9-dc80-499a-a8b4-508c404f0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm(df,mode='train',scaler=''):\n",
    "    \"\"\"\n",
    "    Normalize the data.\n",
    "    df(dataframe) : Input\n",
    "    return tmp(dataframe), scaler : normalized dataframe\n",
    "    \"\"\"\n",
    "    columns = df.columns[1:]\n",
    "\n",
    "    tmp = df.copy()\n",
    "    if mode == 'train':\n",
    "        # Normalize\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(tmp[columns])\n",
    "    elif mode=='test':\n",
    "        scaler = scaler\n",
    "        scaled = scaler.transform(tmp[columns])\n",
    "    # Insert the normalized value to the original frame\n",
    "    tmp[columns] = scaled\n",
    "\n",
    "    return tmp, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c81c5-7df9-4632-93e3-8a278fd95dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_norm(tensor, scaler):\n",
    "    \"\"\"\n",
    "    tensor: [N, T, C] → numpy 배열로 변환 후 역정규화 수행\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    data = tensor.reshape(-1, shape[-1]).cpu().numpy()\n",
    "    inv = scaler.inverse_transform(data)\n",
    "    return inv.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a768ac1-c09f-4aba-8acb-ff87022ded33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_len=10):\n",
    "    \"\"\"\n",
    "    날짜 포함된 시퀀스 생성 함수\n",
    "    df: ['date', 'rn', 'vl', 'wl'] 포함된 DataFrame\n",
    "    반환: (data_seq, date_seq)\n",
    "        - data_seq: torch.Tensor [N, seq_len, 3]\n",
    "        - date_seq: List[List[str]] [N, seq_len]\n",
    "    \"\"\"\n",
    "    cols = df.columns[1:]  # 날짜 제외한 나머지 변수\n",
    "    values = df[cols].values.astype(np.float32)\n",
    "    dates = df['dates'].values  # 문자열 형태로 추출\n",
    "\n",
    "    data_sequences = []\n",
    "    date_sequences = []\n",
    "\n",
    "    for i in range(len(df) - seq_len + 1):\n",
    "        data_seq = values[i:i+seq_len]\n",
    "        date_seq = dates[i:i+seq_len]\n",
    "        data_sequences.extend(data_seq)\n",
    "        date_sequences.extend(date_seq)\n",
    "\n",
    "    return torch.tensor(np.stack(data_sequences)), date_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf7d82-4761-4d4f-b96a-48a76f3b6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sequences(train_dfs, test_dfs, seq_len=120):\n",
    "    \"\"\"\n",
    "    train_dfs(List): Input\n",
    "    test_dfs(List): Input\n",
    "    return train_seq(arr), val_seq(arr): train/val sliding window sequences tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    train_seqs = []\n",
    "    test_seqs = []\n",
    "    train_dates = []\n",
    "    test_dates = []\n",
    "\n",
    "    for train, test in zip(train_dfs, test_dfs):\n",
    "        train_seq, train_date = create_sequences(train, seq_len=seq_len)\n",
    "        test_seq, test_date = create_sequences(test, seq_len=seq_len)\n",
    "        train_seqs.append(train_seq)\n",
    "        train_dates.append(train_date)\n",
    "        test_seqs.append(test_seq)\n",
    "        test_dates.append(test_date)\n",
    "    \n",
    "    return torch.cat(train_seqs, dim=0), torch.cat(test_seqs, dim=0), train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14907a24-b18a-4f21-ac1b-ab5b391dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_seq, val_seq, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create the dataloader.\n",
    "    train_seq(tensor)\n",
    "    val_seq(tensor)\n",
    "    return train_loader, val_loader\n",
    "    \"\"\"\n",
    "    # Make TensorDataset 생성 (Same input and output)\n",
    "    train_dataset = TensorDataset(train_seq, train_seq) # X, y\n",
    "    val_dataset = TensorDataset(val_seq, train_seq) # X, y\n",
    "\n",
    "    # Make DataLoader (Load X,y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b986b-5e4f-4c16-9068-0aa72049a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_usad_lstm(model, train_loader, device='cuda', num_epochs=30, sav_path = '', alpha=0.5, beta=0.5, lr = 1e-4):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Initialize the model with device (CPU, GPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Use the ADAM optimizer\n",
    "    optimizer1 = optim.Adam(list(model.encoder.parameters()) + list(model.decoder1.parameters()), lr=lr)\n",
    "    optimizer2 = optim.Adam(list(model.encoder.parameters()) + list(model.decoder2.parameters()), lr=lr)\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')  # 가장 작은 loss 추적용\n",
    "    save_path = sav_path  # 저장할 폴더\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    loss1_list = []\n",
    "    loss2_list = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss1, epoch_loss2 = 0.0, 0.0\n",
    "\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            # Phase 1\n",
    "            # Initialize the gradient\n",
    "            optimizer1.zero_grad()\n",
    "            # Ouput w1 (_ : w2, z)\n",
    "            w1, _, _ = model(x)\n",
    "            # Calculate Loss function 1\n",
    "            loss1 = alpha * criterion(w1, x)\n",
    "            # Back-propagation\n",
    "            loss1.backward()\n",
    "            # Parameter update with the gradient\n",
    "            optimizer1.step()\n",
    "            epoch_loss1 += loss1.item()\n",
    "\n",
    "            # Phase 2\n",
    "            optimizer2.zero_grad()\n",
    "            # Ouput w1 (_ : w1, z)\n",
    "            _, w2, _ = model(x)\n",
    "            # Calculate Loss function 2\n",
    "            loss2 = beta * criterion(w2, x)\n",
    "            # Back-propagation\n",
    "            loss2.backward()\n",
    "            # Parameter update with the gradient\n",
    "            optimizer2.step()\n",
    "            epoch_loss2 += loss2.item()\n",
    "\n",
    "        avg_loss1 = epoch_loss1 / len(train_loader)\n",
    "        avg_loss2 = epoch_loss2 / len(train_loader)\n",
    "\n",
    "        loss1_list.append(avg_loss1)\n",
    "        loss2_list.append(avg_loss2)\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}] Train Loss1: {epoch_loss1:.6f} | Train Loss2: {epoch_loss2:.6f}\")\n",
    "\n",
    "        # save weights file\n",
    "        if avg_loss1 < best_loss:\n",
    "            best_loss = avg_loss1\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'USAD_best.pth'))\n",
    "            print(f\"[Epoch {epoch}] 모델 저장됨: val_loss = {avg_loss1:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss1_list, label='Loss1 (Encoder+Decoder1)')\n",
    "    plt.plot(loss2_list, label='Loss2 (Encoder+Decoder2)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sav_path, f'USAD_{seq_len}_train_loss_plot.png'))\n",
    "    plt.close()\n",
    "    print(f\"[Saved] Training loss plot saved at {os.path.join(sav_path, f'USAD_{seq_len}_train_loss_plot.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5fb15-ba0b-4fe7-8668-99c9968fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_usad_lstm(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    total_loss1, total_loss2 = 0.0, 0.0\n",
    "    all_x, all_w2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            w1, w2, _ = model(x)\n",
    "\n",
    "            loss1 = criterion(w1, x).item()\n",
    "            loss2 = criterion(w2, x).item()\n",
    "            total_loss1 += loss1\n",
    "            total_loss2 += loss2\n",
    "\n",
    "            all_x.append(x.cpu())\n",
    "            all_w2.append(w2.cpu())\n",
    "\n",
    "    print(f\"[Test] Loss1: {total_loss1:.6f} | Loss2: {total_loss2:.6f}\")\n",
    "    \n",
    "    # 시계열 전체 보정 결과 반환\n",
    "    all_x = torch.cat(all_x, dim=0)    # [N, 60, 3]\n",
    "    all_w2 = torch.cat(all_w2, dim=0)  # [N, 60, 3]\n",
    "    return all_x, all_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf1640-d637-440b-9666-9df461f38521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inverse_transform_column(scaler, normalized_tensor, column_index):\n",
    "    \"\"\"\n",
    "    scaler: fitted MinMaxScaler\n",
    "    normalized_tensor: torch.Tensor of shape [N, T]\n",
    "    column_index: index in original data (0=rn, 1=vl, 2=wl)\n",
    "    \"\"\"\n",
    "    # (1) Tensor → NumPy\n",
    "    arr = normalized_tensor.detach().cpu().numpy()  # shape: [N, T]\n",
    "    N, T = arr.shape\n",
    "\n",
    "    # (2) Flatten to apply scaler: [N*T, 3]\n",
    "    dummy = np.zeros((N*T, 3), dtype=np.float32)\n",
    "    dummy[:, column_index] = arr.flatten()\n",
    "\n",
    "    # (3) Inverse transform\n",
    "    dummy_inv = scaler.inverse_transform(dummy)\n",
    "\n",
    "    # (4) Extract only the column we care about\n",
    "    result = dummy_inv[:, column_index].reshape(N, T)\n",
    "\n",
    "    return result  # shape: [N, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd3eed-9b81-4988-90a2-12735c207b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(x_seq, w2_seq, mode='vl'):\n",
    "    \"\"\"\n",
    "    x_seq, w2_seq: [N, T, 3]\n",
    "    mode: 'vl' → 관로수위만 사용 / 'all' → 전체 변수 평균\n",
    "    반환: [N] shape의 이상치 점수 벡터\n",
    "    \"\"\"\n",
    "    # 관로수위 (index 2)만 비교\n",
    "    errors = (x_seq - w2_seq) ** 2  # [N, T]\n",
    "    scores = errors.mean(dim=1)  # 시퀀스별 평균 MSE → [N]\n",
    "    return errors.numpy(), scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1bf05-ee13-41e6-b75f-b026fb9263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(scores, method='iqr', k=3):\n",
    "    \"\"\"\n",
    "    scores: 이상치 점수 벡터 (numpy)\n",
    "    method: 'iqr', 'mean_std', 'percentile'\n",
    "    \"\"\"\n",
    "    if method == 'iqr':\n",
    "        q1 = np.percentile(scores, 25)\n",
    "        q3 = np.percentile(scores, 75)\n",
    "        iqr = q3 - q1\n",
    "        threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    elif method == 'mean_std':\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        threshold = mean + k * std\n",
    "\n",
    "    elif method == 'percentile':\n",
    "        threshold = np.percentile(scores, 95)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 방식입니다.\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05f9f5-cc0f-4937-ac59-596c0e9b92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corrected_vs_original(original_seq, corrected_seq, input_seq, sav_path = '', sample_idx=0):\n",
    "    \"\"\"\n",
    "    단일 시퀀스 비교 시각화 (60분간)\n",
    "    \"\"\"\n",
    "    orig = original_seq[sample_idx]#.numpy()  # 관로 수위\n",
    "    corr = corrected_seq[sample_idx]#.numpy()\n",
    "    inputs = input_seq[sample_idx]\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(orig, label='Original (vl)', marker='o')\n",
    "    plt.plot(corr, label='Corrected (w2)', marker='x')\n",
    "    plt.plot(inputs, label='Input', marker='+')\n",
    "    plt.title(f'Sample {sample_idx}: Sewer Level Original  vs Corrected')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'{sav_path}/{sample_idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6523c24-aac7-4956-9545-829d997c735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_on_timeseries(original_seq, anomaly_flags, sav_path='', sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plot(scatter) the anomlay detection results on the graph\n",
    "    original_seq : [N, T] or [N, T, 3]\n",
    "    anomaly_flags : [N, T] (bool array)\n",
    "    sample_idx(int)\n",
    "    \"\"\"\n",
    "    vl = original_seq[sample_idx]  # shape: [T]\n",
    "    flags = anomaly_flags[sample_idx]  # shape: [T], bool\n",
    "\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(vl, label='Sewer Level (vl)', color='blue')\n",
    "\n",
    "    # Scatter the anomaly\n",
    "    if flags.any():\n",
    "        plt.scatter(np.where(flags)[0], vl[flags], color='red', label='Detected Anomaly', zorder=3)\n",
    "\n",
    "    plt.title(f'Sample {sample_idx} - Detected {flags.sum()} Anomalies')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{sav_path}/{sample_idx}_anomaly.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f1d9e-fb8f-416b-9744-c024dc85223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_anomaly_statistics(scores, anomaly_flags, original_values=None):\n",
    "    \"\"\"\n",
    "    scores: numpy array [N, T] - 이상치 점수 (MSE 또는 RMSE)\n",
    "    anomaly_flags: bool array [N, T] - 이상치 여부\n",
    "    original_values: optional, numpy array [N, T] - 역정규화된 관측값, 결측치 개수 계산용\n",
    "    \"\"\"\n",
    "    total_points = np.prod(scores.shape)\n",
    "    num_anomalies = np.sum(anomaly_flags)\n",
    "    num_normals = total_points - num_anomalies\n",
    "\n",
    "    # 평균 오차\n",
    "    mean_anomaly_score = np.mean(scores[anomaly_flags]) if num_anomalies > 0 else 0.0\n",
    "    mean_normal_score = np.mean(scores[~anomaly_flags]) if num_normals > 0 else 0.0\n",
    "\n",
    "    # 결측치 개수 계산 (입력된 경우에만)\n",
    "    num_missing = 0\n",
    "    if original_values is not None:\n",
    "        num_missing = np.isnan(original_values).sum()\n",
    "\n",
    "    print(\"📊 이상치 탐지 통계 요약\")\n",
    "    print(f\"- 총 관측 시점 수: {total_points}\")\n",
    "    print(f\"- 이상치 시점 수: {num_anomalies}\")\n",
    "    print(f\"- 이상치 비율: {100.0 * num_anomalies / total_points:.2f}%\")\n",
    "    print(f\"- 이상치 평균 오차: {mean_anomaly_score:.6f}\")\n",
    "    print(f\"- 정상 평균 오차: {mean_normal_score:.6f}\")\n",
    "    if original_values is not None:\n",
    "        print(f\"- 결측치(Missing) 시점 수: {num_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760611e5-da5f-4f86-8bc3-4d4131958f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {0 : 'gwangjoo', 1 : 'changwon', 2 : 'pohang'}\n",
    "fname = {0 : '2920010001045020', 1 : '4812110001018020', 2 : ''}\n",
    "r_cd = 0\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 120\n",
    "input_dim=3\n",
    "hidden_dim=24\n",
    "latent_dim=16\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "data_dir = '../dataset/smart_rain/trainset/sewer/'\n",
    "# 파일 경로 정렬\n",
    "train_paths = sorted(glob.glob(os.path.join(data_dir, 'train*.csv')))\n",
    "test_paths = sorted(glob.glob(os.path.join(data_dir, 'org*.csv')))\n",
    "\n",
    "# 'flag' 열 제외하고 불러오기\n",
    "usecols = ['dates', 'rn', 'vl', 'wl']  # 필요한 컬럼만 명시\n",
    "\n",
    "train_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in train_paths]\n",
    "test_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in test_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b7c41-556f-4fa4-ac9c-91345d33faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 컬럼별 NaN 개수 누적용 (Series 초기화)\n",
    "train_null = pd.Series(0, index=train_dfs[0].columns)\n",
    "test_null = pd.Series(0, index=test_dfs[0].columns)\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    train_null += train.isnull().sum()\n",
    "    test_null += test.isnull().sum()\n",
    "\n",
    "print('Train set NaN 개수 (컬럼별):\\n', train_null)\n",
    "print('Test set NaN 개수 (컬럼별):\\n', test_null)\n",
    "\n",
    "# 전체 NaN 총합 출력\n",
    "print(f\"\\nTrain set 전체 NaN 수: {train_null.sum()}\")\n",
    "print(f\"Test set 전체 NaN 수: {test_null.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdffd2-f293-4701-8cfd-ade76164e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm_dfs = []\n",
    "test_norm_dfs = []\n",
    "_, train_scaler = data_norm(pd.concat(train_dfs))\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    test.fillna(0.0, inplace=True)\n",
    "    \n",
    "    train_norm, _ = data_norm(train, scaler = train_scaler)\n",
    "    test_norm, _ = data_norm(test, mode='test',scaler=train_scaler)\n",
    "    \n",
    "    test_norm['vl'] = test_norm['vl'].clip(lower= 0.0)\n",
    "    \n",
    "    train_norm_dfs.append(train_norm)\n",
    "    test_norm_dfs.append(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76977f1d-9cc4-40c0-9436-4f78070a936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('▶ Train set')\n",
    "tmp_orig = pd.concat(train_dfs)\n",
    "tmp_norm = pd.concat(train_norm_dfs)\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_orig['rn'].min(), tmp_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_norm['rn'].min(), tmp_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_orig['vl'].min(), tmp_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_norm['vl'].min(), tmp_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_orig['wl'].min(), tmp_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_norm['wl'].min(), tmp_norm['wl'].max())\n",
    "\n",
    "print('\\n▶ Test set')\n",
    "tmp_test_orig = pd.concat(test_dfs)\n",
    "tmp_test_norm = pd.concat(test_norm_dfs)\n",
    "\n",
    "print('Test set Nan count:\\n', tmp_test_orig.isnull().sum())\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_test_orig['rn'].min(), tmp_test_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_test_norm['rn'].min(), tmp_test_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_test_orig['vl'].min(), tmp_test_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_test_norm['vl'].min(), tmp_test_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_test_orig['wl'].min(), tmp_test_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_test_norm['wl'].min(), tmp_test_norm['wl'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13272cf-2a49-42bd-aec0-d8e976ada5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, val_seq, train_date, test_date = create_combined_sequences(train_norm_dfs, test_norm_dfs, seq_len=seq_len)\n",
    "\n",
    "print(\"Train shape:\", train_seq.shape)  # (N_train, 60, 3)\n",
    "print(\"Val shape:\", val_seq.shape)      # (N_val, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f643a0-e260-4802-bde2-75c355b44c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = np.stack(train_date)\n",
    "test_date = np.stack(test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684f2fb-5088-413d-81f2-6a0a3758bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617089c-0cc4-4388-9ab2-4fd2dc715194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = USAD_LSTM(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim, seq_len=seq_len)\n",
    "example = torch.randn((batch_size,seq_len, input_dim)) \n",
    "w1, w2, z = model(example)\n",
    "\n",
    "print(w1.shape)  # torch.Size([32, 10, 3])\n",
    "print(w2.shape)  # torch.Size([32, 10, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ecfa7-d608-4c0d-9ea1-13d0d18a0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(example.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ee97a-efb1-44ec-b6c6-7ddc2f9325e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loader to load the dataset\n",
    "train_loader, val_loader = create_dataloaders(train_seq, val_seq, batch_size=batch_size)\n",
    "\n",
    "# Check the dataset shape\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)  \n",
    "    print(y.shape)  \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953fe60-d801-48df-815b-d506472d84f9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12615f56-be0c-4a02-832a-49122f13838e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Start\n",
    "train_usad_lstm(model, train_loader, device='cpu', num_epochs=epochs,sav_path=f'./sav/USAD_{seq_len}_rn_vl_wl/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c285b7e-4dda-4a1e-b298-aec09c3b1737",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6459e-2306-4ca7-8bbf-133cd4034f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./sav/USAD_{seq_len}_rn_vl_wl/USAD_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34168dbb-be1b-4d88-8605-86893c160e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test\n",
    "original_seq, corrected_seq = test_usad_lstm(model, val_loader, device='cpu')\n",
    "\n",
    "# Select the correted sequence only sewer level\"\n",
    "original_vl =  original_seq[:, :, 1]  # [N, 60]\n",
    "corrected_vl = corrected_seq[:, :, 1]  # [N, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e818adf-71fb-42d4-ab74-31a16f5cf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate the anomaly scores\n",
    "errors, scores = compute_anomaly_scores(original_vl, corrected_vl, mode='vl')  # 관로수위 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24774e-4d98-4038-b224-f3a6c14e12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the threshold\n",
    "threshold = determine_threshold(scores, method='iqr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252a3fc-9f7d-4be7-864e-b968f05fdc0f",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(scores)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "plt.title(\"Anomaly Scores Over Time (Each Sequence)\")\n",
    "plt.xlabel(\"Sequence Index\")\n",
    "plt.ylabel(\"Anomaly Score\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5518d03-9d67-430f-b0cd-2754c68e1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 이상치 탐지\n",
    "anomaly_flags = errors > threshold\n",
    "\n",
    "print(\"임계값:\", threshold)\n",
    "print(\"탐지된 이상치 수:\", np.sum(anomaly_flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98c153-f9a5-479c-84d0-56f07cb848ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sewer_lv = inverse_transform_column(train_scaler, original_vl, column_index=1) \n",
    "corrected_norm = inverse_transform_column(train_scaler, corrected_vl, column_index=1) \n",
    "corrected_norm[corrected_norm<0] = 0.0\n",
    "input_seq = inverse_transform_column(train_scaler, val_seq[:,:,1], column_index=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d1e77-f2ef-4def-b592-e949b5276eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_norm = corrected_norm.round()\n",
    "input_seq = input_seq.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69edd88-266e-4811-bc9e-dff9fe499db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sav_path = f'./imgs/USAD_{seq_len}/'\n",
    "for i in range(50):\n",
    "    n = random.randint(0,len(val_seq))\n",
    "    plot_corrected_vs_original(sewer_lv, corrected_norm, input_seq, sav_path, sample_idx=n)\n",
    "    plot_anomalies_on_timeseries(sewer_lv, anomaly_flags, sav_path, sample_idx=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565335a-63a8-4cf3-9319-8fdd7896d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_anomaly_statistics(errors, anomaly_flags, original_values=original_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fdbe1-1f37-49cc-a089-2cd03828d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_anomaly_mask(original_seq, anomaly_flags, mask_value=np.nan, target_idx=1):\n",
    "    \"\"\"\n",
    "    original_seq: numpy array [N, T, D] - 원본 시계열 데이터\n",
    "    anomaly_flags: bool array [N, T] - 이상치 위치 (True: 이상치)\n",
    "    mask_value: 마스킹할 값 (기본값: np.nan)\n",
    "    target_idx: 마스킹할 변수 index (기본: 관로수위 = 1)\n",
    "    return: 마스킹된 시계열 데이터 (복사본)\n",
    "    \"\"\"\n",
    "    masked_seq = original_seq.copy()\n",
    "    masked_seq[:, :, target_idx][anomaly_flags] = mask_value\n",
    "    return masked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb262177-7ccd-4353-bb99-94085a16d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_seq = apply_anomaly_mask(original_seq.numpy(), anomaly_flags, mask_value=np.nan, target_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616131-c1e4-4cf6-81bb-484369cf84bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
