{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3983f-a7ad-4eb9-83c9-1c34cb9b9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6f2bf-d6f3-4836-be76-562ee67f5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True) # input dim (3) -> hidden dim (LSTM)\n",
    "        self.fc = nn.Linear(hidden_dim, latent_dim) # hidden dim -> latent dim (z) (FC layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        h, _ = self.lstm(x)  # h: [B, T, H]\n",
    "        h_last = h[:, -1, :]  # Last hidden state (sequence summary)\n",
    "        z = self.fc(h_last)  # [B, latent_dim]\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002e835-e8d7-4757-b323-8f49f227e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, seq_len=10, num_layers=1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim) # latent dim -> hidden dim (FC layer)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True) #hidden dim -> hidden dim (LSTM)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # hidden dim -> output dim (3) (FC layer)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, latent_dim]\n",
    "        h0 = self.latent_to_hidden(z)  # latent vector [B, H] to hidden state\n",
    "        h0_seq = h0.unsqueeze(1).repeat(1, self.seq_len, 1)  # Copy for the sequence length (T) [B, T, H]\n",
    "        h_seq, _ = self.lstm(h0_seq) # Inference for 1 step (Make sequence)\n",
    "        out = self.fc(h_seq)  # Recover for 3dim var with each time step output [B, T, output_dim]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834838c9-402b-4911-a51a-ba47afa2bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USAD_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, seq_len=10):\n",
    "        super(USAD_LSTM, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder1 = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_len)\n",
    "        self.decoder2 = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        return w1, w2, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac2cc9-dc80-499a-a8b4-508c404f0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm(df,mode='train',scaler=''):\n",
    "    \"\"\"\n",
    "    Normalize the data.\n",
    "    df(dataframe) : Input\n",
    "    return tmp(dataframe), scaler : normalized dataframe\n",
    "    \"\"\"\n",
    "    columns = df.columns[1:]\n",
    "\n",
    "    tmp = df.copy()\n",
    "    if mode == 'train':\n",
    "        # Normalize\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(tmp[columns])\n",
    "    elif mode=='test':\n",
    "        scaler = scaler\n",
    "        scaled = scaler.transform(tmp[columns])\n",
    "    # Insert the normalized value to the original frame\n",
    "    tmp[columns] = scaled\n",
    "\n",
    "    return tmp, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c81c5-7df9-4632-93e3-8a278fd95dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_norm(tensor, scaler):\n",
    "    \"\"\"\n",
    "    tensor: [N, T, C] â†’ numpy ë°°ì—´ë¡œ ë³€í™˜ í›„ ì—­ì •ê·œí™” ìˆ˜í–‰\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    data = tensor.reshape(-1, shape[-1]).cpu().numpy()\n",
    "    inv = scaler.inverse_transform(data)\n",
    "    return inv.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a768ac1-c09f-4aba-8acb-ff87022ded33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_len=10):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ í¬í•¨ëœ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "    df: ['date', 'rn', 'vl', 'wl'] í¬í•¨ëœ DataFrame\n",
    "    ë°˜í™˜: (data_seq, date_seq)\n",
    "        - data_seq: torch.Tensor [N, seq_len, 3]\n",
    "        - date_seq: List[List[str]] [N, seq_len]\n",
    "    \"\"\"\n",
    "    cols = df.columns[1:]  # ë‚ ì§œ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜\n",
    "    values = df[cols].values.astype(np.float32)\n",
    "    dates = df['dates'].values  # ë¬¸ìì—´ í˜•íƒœë¡œ ì¶”ì¶œ\n",
    "\n",
    "    data_sequences = []\n",
    "    date_sequences = []\n",
    "\n",
    "    for i in range(len(df) - seq_len + 1):\n",
    "        data_seq = values[i:i+seq_len]\n",
    "        date_seq = dates[i:i+seq_len]\n",
    "        data_sequences.extend(data_seq)\n",
    "        date_sequences.extend(date_seq)\n",
    "\n",
    "    return torch.tensor(np.stack(data_sequences)), date_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf7d82-4761-4d4f-b96a-48a76f3b6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sequences(train_dfs, test_dfs, seq_len=120):\n",
    "    \"\"\"\n",
    "    train_dfs(List): Input\n",
    "    test_dfs(List): Input\n",
    "    return train_seq(arr), val_seq(arr): train/val sliding window sequences tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    train_seqs = []\n",
    "    test_seqs = []\n",
    "    train_dates = []\n",
    "    test_dates = []\n",
    "\n",
    "    for train, test in zip(train_dfs, test_dfs):\n",
    "        train_seq, train_date = create_sequences(train, seq_len=seq_len)\n",
    "        test_seq, test_date = create_sequences(test, seq_len=seq_len)\n",
    "        train_seqs.append(train_seq)\n",
    "        train_dates.append(train_date)\n",
    "        test_seqs.append(test_seq)\n",
    "        test_dates.append(test_date)\n",
    "    \n",
    "    return torch.cat(train_seqs, dim=0), torch.cat(test_seqs, dim=0), train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14907a24-b18a-4f21-ac1b-ab5b391dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_seq, val_seq, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create the dataloader.\n",
    "    train_seq(tensor)\n",
    "    val_seq(tensor)\n",
    "    return train_loader, val_loader\n",
    "    \"\"\"\n",
    "    # Make TensorDataset ìƒì„± (Same input and output)\n",
    "    train_dataset = TensorDataset(train_seq, train_seq) # X, y\n",
    "    val_dataset = TensorDataset(val_seq, train_seq) # X, y\n",
    "\n",
    "    # Make DataLoader (Load X,y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b986b-5e4f-4c16-9068-0aa72049a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_usad_lstm(model, train_loader, device='cuda', num_epochs=30, sav_path = '', alpha=0.5, beta=0.5, lr = 1e-4):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Initialize the model with device (CPU, GPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Use the ADAM optimizer\n",
    "    optimizer1 = optim.Adam(list(model.encoder.parameters()) + list(model.decoder1.parameters()), lr=lr)\n",
    "    optimizer2 = optim.Adam(list(model.encoder.parameters()) + list(model.decoder2.parameters()), lr=lr)\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')  # ê°€ì¥ ì‘ì€ loss ì¶”ì ìš©\n",
    "    save_path = sav_path  # ì €ì¥í•  í´ë”\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    loss1_list = []\n",
    "    loss2_list = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss1, epoch_loss2 = 0.0, 0.0\n",
    "\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            # Phase 1\n",
    "            # Initialize the gradient\n",
    "            optimizer1.zero_grad()\n",
    "            # Ouput w1 (_ : w2, z)\n",
    "            w1, _, _ = model(x)\n",
    "            # Calculate Loss function 1\n",
    "            loss1 = alpha * criterion(w1, x)\n",
    "            # Back-propagation\n",
    "            loss1.backward()\n",
    "            # Parameter update with the gradient\n",
    "            optimizer1.step()\n",
    "            epoch_loss1 += loss1.item()\n",
    "\n",
    "            # Phase 2\n",
    "            optimizer2.zero_grad()\n",
    "            # Ouput w1 (_ : w1, z)\n",
    "            _, w2, _ = model(x)\n",
    "            # Calculate Loss function 2\n",
    "            loss2 = beta * criterion(w2, x)\n",
    "            # Back-propagation\n",
    "            loss2.backward()\n",
    "            # Parameter update with the gradient\n",
    "            optimizer2.step()\n",
    "            epoch_loss2 += loss2.item()\n",
    "\n",
    "        avg_loss1 = epoch_loss1 / len(train_loader)\n",
    "        avg_loss2 = epoch_loss2 / len(train_loader)\n",
    "\n",
    "        loss1_list.append(avg_loss1)\n",
    "        loss2_list.append(avg_loss2)\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}] Train Loss1: {epoch_loss1:.6f} | Train Loss2: {epoch_loss2:.6f}\")\n",
    "\n",
    "        # save weights file\n",
    "        if avg_loss1 < best_loss:\n",
    "            best_loss = avg_loss1\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'USAD_best.pth'))\n",
    "            print(f\"[Epoch {epoch}] ëª¨ë¸ ì €ì¥ë¨: val_loss = {avg_loss1:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss1_list, label='Loss1 (Encoder+Decoder1)')\n",
    "    plt.plot(loss2_list, label='Loss2 (Encoder+Decoder2)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sav_path, f'USAD_{seq_len}_train_loss_plot.png'))\n",
    "    plt.close()\n",
    "    print(f\"[Saved] Training loss plot saved at {os.path.join(sav_path, f'USAD_{seq_len}_train_loss_plot.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5fb15-ba0b-4fe7-8668-99c9968fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_usad_lstm(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    total_loss1, total_loss2 = 0.0, 0.0\n",
    "    all_x, all_w2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            w1, w2, _ = model(x)\n",
    "\n",
    "            loss1 = criterion(w1, x).item()\n",
    "            loss2 = criterion(w2, x).item()\n",
    "            total_loss1 += loss1\n",
    "            total_loss2 += loss2\n",
    "\n",
    "            all_x.append(x.cpu())\n",
    "            all_w2.append(w2.cpu())\n",
    "\n",
    "    print(f\"[Test] Loss1: {total_loss1:.6f} | Loss2: {total_loss2:.6f}\")\n",
    "    \n",
    "    # ì‹œê³„ì—´ ì „ì²´ ë³´ì • ê²°ê³¼ ë°˜í™˜\n",
    "    all_x = torch.cat(all_x, dim=0)    # [N, 60, 3]\n",
    "    all_w2 = torch.cat(all_w2, dim=0)  # [N, 60, 3]\n",
    "    return all_x, all_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf1640-d637-440b-9666-9df461f38521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inverse_transform_column(scaler, normalized_tensor, column_index):\n",
    "    \"\"\"\n",
    "    scaler: fitted MinMaxScaler\n",
    "    normalized_tensor: torch.Tensor of shape [N, T]\n",
    "    column_index: index in original data (0=rn, 1=vl, 2=wl)\n",
    "    \"\"\"\n",
    "    # (1) Tensor â†’ NumPy\n",
    "    arr = normalized_tensor.detach().cpu().numpy()  # shape: [N, T]\n",
    "    N, T = arr.shape\n",
    "\n",
    "    # (2) Flatten to apply scaler: [N*T, 3]\n",
    "    dummy = np.zeros((N*T, 3), dtype=np.float32)\n",
    "    dummy[:, column_index] = arr.flatten()\n",
    "\n",
    "    # (3) Inverse transform\n",
    "    dummy_inv = scaler.inverse_transform(dummy)\n",
    "\n",
    "    # (4) Extract only the column we care about\n",
    "    result = dummy_inv[:, column_index].reshape(N, T)\n",
    "\n",
    "    return result  # shape: [N, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd3eed-9b81-4988-90a2-12735c207b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(x_seq, w2_seq, mode='vl'):\n",
    "    \"\"\"\n",
    "    x_seq, w2_seq: [N, T, 3]\n",
    "    mode: 'vl' â†’ ê´€ë¡œìˆ˜ìœ„ë§Œ ì‚¬ìš© / 'all' â†’ ì „ì²´ ë³€ìˆ˜ í‰ê· \n",
    "    ë°˜í™˜: [N] shapeì˜ ì´ìƒì¹˜ ì ìˆ˜ ë²¡í„°\n",
    "    \"\"\"\n",
    "    # ê´€ë¡œìˆ˜ìœ„ (index 2)ë§Œ ë¹„êµ\n",
    "    errors = (x_seq - w2_seq) ** 2  # [N, T]\n",
    "    scores = errors.mean(dim=1)  # ì‹œí€€ìŠ¤ë³„ í‰ê·  MSE â†’ [N]\n",
    "    return errors.numpy(), scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1bf05-ee13-41e6-b75f-b026fb9263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(scores, method='iqr', k=3):\n",
    "    \"\"\"\n",
    "    scores: ì´ìƒì¹˜ ì ìˆ˜ ë²¡í„° (numpy)\n",
    "    method: 'iqr', 'mean_std', 'percentile'\n",
    "    \"\"\"\n",
    "    if method == 'iqr':\n",
    "        q1 = np.percentile(scores, 25)\n",
    "        q3 = np.percentile(scores, 75)\n",
    "        iqr = q3 - q1\n",
    "        threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    elif method == 'mean_std':\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        threshold = mean + k * std\n",
    "\n",
    "    elif method == 'percentile':\n",
    "        threshold = np.percentile(scores, 95)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05f9f5-cc0f-4937-ac59-596c0e9b92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corrected_vs_original(original_seq, corrected_seq, input_seq, sav_path = '', sample_idx=0):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ì‹œí€€ìŠ¤ ë¹„êµ ì‹œê°í™” (60ë¶„ê°„)\n",
    "    \"\"\"\n",
    "    orig = original_seq[sample_idx]#.numpy()  # ê´€ë¡œ ìˆ˜ìœ„\n",
    "    corr = corrected_seq[sample_idx]#.numpy()\n",
    "    inputs = input_seq[sample_idx]\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(orig, label='Original (vl)', marker='o')\n",
    "    plt.plot(corr, label='Corrected (w2)', marker='x')\n",
    "    plt.plot(inputs, label='Input', marker='+')\n",
    "    plt.title(f'Sample {sample_idx}: Sewer Level Original  vs Corrected')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'{sav_path}/{sample_idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6523c24-aac7-4956-9545-829d997c735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_on_timeseries(original_seq, anomaly_flags, sav_path='', sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plot(scatter) the anomlay detection results on the graph\n",
    "    original_seq : [N, T] or [N, T, 3]\n",
    "    anomaly_flags : [N, T] (bool array)\n",
    "    sample_idx(int)\n",
    "    \"\"\"\n",
    "    vl = original_seq[sample_idx]  # shape: [T]\n",
    "    flags = anomaly_flags[sample_idx]  # shape: [T], bool\n",
    "\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(vl, label='Sewer Level (vl)', color='blue')\n",
    "\n",
    "    # Scatter the anomaly\n",
    "    if flags.any():\n",
    "        plt.scatter(np.where(flags)[0], vl[flags], color='red', label='Detected Anomaly', zorder=3)\n",
    "\n",
    "    plt.title(f'Sample {sample_idx} - Detected {flags.sum()} Anomalies')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{sav_path}/{sample_idx}_anomaly.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f1d9e-fb8f-416b-9744-c024dc85223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_anomaly_statistics(scores, anomaly_flags, original_values=None):\n",
    "    \"\"\"\n",
    "    scores: numpy array [N, T] - ì´ìƒì¹˜ ì ìˆ˜ (MSE ë˜ëŠ” RMSE)\n",
    "    anomaly_flags: bool array [N, T] - ì´ìƒì¹˜ ì—¬ë¶€\n",
    "    original_values: optional, numpy array [N, T] - ì—­ì •ê·œí™”ëœ ê´€ì¸¡ê°’, ê²°ì¸¡ì¹˜ ê°œìˆ˜ ê³„ì‚°ìš©\n",
    "    \"\"\"\n",
    "    total_points = np.prod(scores.shape)\n",
    "    num_anomalies = np.sum(anomaly_flags)\n",
    "    num_normals = total_points - num_anomalies\n",
    "\n",
    "    # í‰ê·  ì˜¤ì°¨\n",
    "    mean_anomaly_score = np.mean(scores[anomaly_flags]) if num_anomalies > 0 else 0.0\n",
    "    mean_normal_score = np.mean(scores[~anomaly_flags]) if num_normals > 0 else 0.0\n",
    "\n",
    "    # ê²°ì¸¡ì¹˜ ê°œìˆ˜ ê³„ì‚° (ì…ë ¥ëœ ê²½ìš°ì—ë§Œ)\n",
    "    num_missing = 0\n",
    "    if original_values is not None:\n",
    "        num_missing = np.isnan(original_values).sum()\n",
    "\n",
    "    print(\"ğŸ“Š ì´ìƒì¹˜ íƒì§€ í†µê³„ ìš”ì•½\")\n",
    "    print(f\"- ì´ ê´€ì¸¡ ì‹œì  ìˆ˜: {total_points}\")\n",
    "    print(f\"- ì´ìƒì¹˜ ì‹œì  ìˆ˜: {num_anomalies}\")\n",
    "    print(f\"- ì´ìƒì¹˜ ë¹„ìœ¨: {100.0 * num_anomalies / total_points:.2f}%\")\n",
    "    print(f\"- ì´ìƒì¹˜ í‰ê·  ì˜¤ì°¨: {mean_anomaly_score:.6f}\")\n",
    "    print(f\"- ì •ìƒ í‰ê·  ì˜¤ì°¨: {mean_normal_score:.6f}\")\n",
    "    if original_values is not None:\n",
    "        print(f\"- ê²°ì¸¡ì¹˜(Missing) ì‹œì  ìˆ˜: {num_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760611e5-da5f-4f86-8bc3-4d4131958f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {0 : 'gwangjoo', 1 : 'changwon', 2 : 'pohang'}\n",
    "fname = {0 : '2920010001045020', 1 : '4812110001018020', 2 : ''}\n",
    "r_cd = 0\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 120\n",
    "input_dim=3\n",
    "hidden_dim=24\n",
    "latent_dim=16\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "data_dir = '../dataset/smart_rain/trainset/sewer/'\n",
    "# íŒŒì¼ ê²½ë¡œ ì •ë ¬\n",
    "train_paths = sorted(glob.glob(os.path.join(data_dir, 'train*.csv')))\n",
    "test_paths = sorted(glob.glob(os.path.join(data_dir, 'org*.csv')))\n",
    "\n",
    "# 'flag' ì—´ ì œì™¸í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "usecols = ['dates', 'rn', 'vl', 'wl']  # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ëª…ì‹œ\n",
    "\n",
    "train_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in train_paths]\n",
    "test_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in test_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b7c41-556f-4fa4-ac9c-91345d33faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ì»¬ëŸ¼ë³„ NaN ê°œìˆ˜ ëˆ„ì ìš© (Series ì´ˆê¸°í™”)\n",
    "train_null = pd.Series(0, index=train_dfs[0].columns)\n",
    "test_null = pd.Series(0, index=test_dfs[0].columns)\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    train_null += train.isnull().sum()\n",
    "    test_null += test.isnull().sum()\n",
    "\n",
    "print('Train set NaN ê°œìˆ˜ (ì»¬ëŸ¼ë³„):\\n', train_null)\n",
    "print('Test set NaN ê°œìˆ˜ (ì»¬ëŸ¼ë³„):\\n', test_null)\n",
    "\n",
    "# ì „ì²´ NaN ì´í•© ì¶œë ¥\n",
    "print(f\"\\nTrain set ì „ì²´ NaN ìˆ˜: {train_null.sum()}\")\n",
    "print(f\"Test set ì „ì²´ NaN ìˆ˜: {test_null.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdffd2-f293-4701-8cfd-ade76164e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm_dfs = []\n",
    "test_norm_dfs = []\n",
    "_, train_scaler = data_norm(pd.concat(train_dfs))\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    test.fillna(0.0, inplace=True)\n",
    "    \n",
    "    train_norm, _ = data_norm(train, scaler = train_scaler)\n",
    "    test_norm, _ = data_norm(test, mode='test',scaler=train_scaler)\n",
    "    \n",
    "    test_norm['vl'] = test_norm['vl'].clip(lower= 0.0)\n",
    "    \n",
    "    train_norm_dfs.append(train_norm)\n",
    "    test_norm_dfs.append(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76977f1d-9cc4-40c0-9436-4f78070a936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('â–¶ Train set')\n",
    "tmp_orig = pd.concat(train_dfs)\n",
    "tmp_norm = pd.concat(train_norm_dfs)\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_orig['rn'].min(), tmp_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_norm['rn'].min(), tmp_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_orig['vl'].min(), tmp_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_norm['vl'].min(), tmp_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_orig['wl'].min(), tmp_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_norm['wl'].min(), tmp_norm['wl'].max())\n",
    "\n",
    "print('\\nâ–¶ Test set')\n",
    "tmp_test_orig = pd.concat(test_dfs)\n",
    "tmp_test_norm = pd.concat(test_norm_dfs)\n",
    "\n",
    "print('Test set Nan count:\\n', tmp_test_orig.isnull().sum())\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_test_orig['rn'].min(), tmp_test_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_test_norm['rn'].min(), tmp_test_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_test_orig['vl'].min(), tmp_test_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_test_norm['vl'].min(), tmp_test_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_test_orig['wl'].min(), tmp_test_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_test_norm['wl'].min(), tmp_test_norm['wl'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13272cf-2a49-42bd-aec0-d8e976ada5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, val_seq, train_date, test_date = create_combined_sequences(train_norm_dfs, test_norm_dfs, seq_len=seq_len)\n",
    "\n",
    "print(\"Train shape:\", train_seq.shape)  # (N_train, 60, 3)\n",
    "print(\"Val shape:\", val_seq.shape)      # (N_val, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f643a0-e260-4802-bde2-75c355b44c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = np.stack(train_date)\n",
    "test_date = np.stack(test_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684f2fb-5088-413d-81f2-6a0a3758bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617089c-0cc4-4388-9ab2-4fd2dc715194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = USAD_LSTM(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim, seq_len=seq_len)\n",
    "example = torch.randn((batch_size,seq_len, input_dim)) \n",
    "w1, w2, z = model(example)\n",
    "\n",
    "print(w1.shape)  # torch.Size([32, 10, 3])\n",
    "print(w2.shape)  # torch.Size([32, 10, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ecfa7-d608-4c0d-9ea1-13d0d18a0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(example.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ee97a-efb1-44ec-b6c6-7ddc2f9325e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loader to load the dataset\n",
    "train_loader, val_loader = create_dataloaders(train_seq, val_seq, batch_size=batch_size)\n",
    "\n",
    "# Check the dataset shape\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)  \n",
    "    print(y.shape)  \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953fe60-d801-48df-815b-d506472d84f9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12615f56-be0c-4a02-832a-49122f13838e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Start\n",
    "train_usad_lstm(model, train_loader, device='cpu', num_epochs=epochs,sav_path=f'./sav/USAD_{seq_len}_rn_vl_wl/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c285b7e-4dda-4a1e-b298-aec09c3b1737",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6459e-2306-4ca7-8bbf-133cd4034f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./sav/USAD_{seq_len}_rn_vl_wl/USAD_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34168dbb-be1b-4d88-8605-86893c160e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test\n",
    "original_seq, corrected_seq = test_usad_lstm(model, val_loader, device='cpu')\n",
    "\n",
    "# Select the correted sequence only sewer level\"\n",
    "original_vl =  original_seq[:, :, 1]  # [N, 60]\n",
    "corrected_vl = corrected_seq[:, :, 1]  # [N, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e818adf-71fb-42d4-ab74-31a16f5cf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate the anomaly scores\n",
    "errors, scores = compute_anomaly_scores(original_vl, corrected_vl, mode='vl')  # ê´€ë¡œìˆ˜ìœ„ ê¸°ì¤€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24774e-4d98-4038-b224-f3a6c14e12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the threshold\n",
    "threshold = determine_threshold(scores, method='iqr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252a3fc-9f7d-4be7-864e-b968f05fdc0f",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(scores)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "plt.title(\"Anomaly Scores Over Time (Each Sequence)\")\n",
    "plt.xlabel(\"Sequence Index\")\n",
    "plt.ylabel(\"Anomaly Score\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5518d03-9d67-430f-b0cd-2754c68e1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ì´ìƒì¹˜ íƒì§€\n",
    "anomaly_flags = errors > threshold\n",
    "\n",
    "print(\"ì„ê³„ê°’:\", threshold)\n",
    "print(\"íƒì§€ëœ ì´ìƒì¹˜ ìˆ˜:\", np.sum(anomaly_flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98c153-f9a5-479c-84d0-56f07cb848ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sewer_lv = inverse_transform_column(train_scaler, original_vl, column_index=1) \n",
    "corrected_norm = inverse_transform_column(train_scaler, corrected_vl, column_index=1) \n",
    "corrected_norm[corrected_norm<0] = 0.0\n",
    "input_seq = inverse_transform_column(train_scaler, val_seq[:,:,1], column_index=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d1e77-f2ef-4def-b592-e949b5276eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_norm = corrected_norm.round()\n",
    "input_seq = input_seq.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69edd88-266e-4811-bc9e-dff9fe499db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sav_path = f'./imgs/USAD_{seq_len}/'\n",
    "for i in range(50):\n",
    "    n = random.randint(0,len(val_seq))\n",
    "    plot_corrected_vs_original(sewer_lv, corrected_norm, input_seq, sav_path, sample_idx=n)\n",
    "    plot_anomalies_on_timeseries(sewer_lv, anomaly_flags, sav_path, sample_idx=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565335a-63a8-4cf3-9319-8fdd7896d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_anomaly_statistics(errors, anomaly_flags, original_values=original_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fdbe1-1f37-49cc-a089-2cd03828d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_anomaly_mask(original_seq, anomaly_flags, mask_value=np.nan, target_idx=1):\n",
    "    \"\"\"\n",
    "    original_seq: numpy array [N, T, D] - ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°\n",
    "    anomaly_flags: bool array [N, T] - ì´ìƒì¹˜ ìœ„ì¹˜ (True: ì´ìƒì¹˜)\n",
    "    mask_value: ë§ˆìŠ¤í‚¹í•  ê°’ (ê¸°ë³¸ê°’: np.nan)\n",
    "    target_idx: ë§ˆìŠ¤í‚¹í•  ë³€ìˆ˜ index (ê¸°ë³¸: ê´€ë¡œìˆ˜ìœ„ = 1)\n",
    "    return: ë§ˆìŠ¤í‚¹ëœ ì‹œê³„ì—´ ë°ì´í„° (ë³µì‚¬ë³¸)\n",
    "    \"\"\"\n",
    "    masked_seq = original_seq.copy()\n",
    "    masked_seq[:, :, target_idx][anomaly_flags] = mask_value\n",
    "    return masked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb262177-7ccd-4353-bb99-94085a16d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_seq = apply_anomaly_mask(original_seq.numpy(), anomaly_flags, mask_value=np.nan, target_idx=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616131-c1e4-4cf6-81bb-484369cf84bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
