{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3983f-a7ad-4eb9-83c9-1c34cb9b9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c45caf-46c4-4153-8083-9ccea86cb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57720ead-73fd-410a-83db-b2c3b0203b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6f2bf-d6f3-4836-be76-562ee67f5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True) # input dim (3) -> hidden dim (LSTM)\n",
    "        self.fc = nn.Linear(hidden_dim, latent_dim) # hidden dim -> latent dim (z) (FC layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        h, _ = self.lstm(x)  # h: [B, T, H]\n",
    "        h_last = h[:, -1, :]  # Last hidden state (sequence summary)\n",
    "        z = self.fc(h_last)  # [B, latent_dim]\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002e835-e8d7-4757-b323-8f49f227e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, seq_len=10, num_layers=1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim) # latent dim -> hidden dim (FC layer)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True) #hidden dim -> hidden dim (LSTM)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # hidden dim -> output dim (3) (FC layer)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, latent_dim]\n",
    "        h0 = self.latent_to_hidden(z)  # latent vector [B, H] to hidden state\n",
    "        h0_seq = h0.unsqueeze(1).repeat(1, self.seq_len, 1)  # Copy for the sequence length (T) [B, T, H]\n",
    "        h_seq, _ = self.lstm(h0_seq) # Inference for 1 step (Make sequence)\n",
    "        out = self.fc(h_seq)  # Recover for 3dim var with each time step output [B, T, output_dim]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834838c9-402b-4911-a51a-ba47afa2bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USAD_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, seq_len=10):\n",
    "        super(USAD_LSTM, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder1 = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_len)\n",
    "        self.decoder2 = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        return w1, w2, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac2cc9-dc80-499a-a8b4-508c404f0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm(df,mode='train',scaler=''):\n",
    "    \"\"\"\n",
    "    Normalize the data.\n",
    "    df(dataframe) : Input\n",
    "    return tmp(dataframe), scaler : normalized dataframe\n",
    "    \"\"\"\n",
    "    columns = df.columns[1:]\n",
    "\n",
    "    tmp = df.copy()\n",
    "    if mode == 'train':\n",
    "        # Normalize\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(tmp[columns])\n",
    "    elif mode=='test':\n",
    "        scaler = scaler\n",
    "        scaled = scaler.transform(tmp[columns])\n",
    "    # Insert the normalized value to the original frame\n",
    "    tmp[columns] = scaled\n",
    "\n",
    "    return tmp, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c81c5-7df9-4632-93e3-8a278fd95dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_norm(tensor, scaler):\n",
    "    \"\"\"\n",
    "    tensor: [N, T, C] â†’ numpy ë°°ì—´ë¡œ ë³€í™˜ í›„ ì—­ì •ê·œí™” ìˆ˜í–‰\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    data = tensor.reshape(-1, shape[-1])\n",
    "    inv = scaler.inverse_transform(data)\n",
    "    return inv.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a768ac1-c09f-4aba-8acb-ff87022ded33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_len=10):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œ í¬í•¨ëœ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "    df: ['date', 'rn', 'vl', 'wl'] í¬í•¨ëœ DataFrame\n",
    "    ë°˜í™˜: (data_seq, date_seq)\n",
    "        - data_seq: torch.Tensor [N, seq_len, 3]\n",
    "        - date_seq: List[List[str]] [N, seq_len]\n",
    "    \"\"\"\n",
    "    cols = df.columns[1:]  # ë‚ ì§œ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜\n",
    "    values = df[cols].values.astype(np.float32)\n",
    "    dates = df['dates'].values  # ë¬¸ìì—´ í˜•íƒœë¡œ ì¶”ì¶œ\n",
    "\n",
    "    data_sequences = []\n",
    "    date_sequences = []\n",
    "\n",
    "    for i in range(len(df) - seq_len + 1):\n",
    "        data_seq = values[i:i+seq_len]\n",
    "        date_seq = dates[i:i+seq_len]\n",
    "        data_sequences.append(data_seq)\n",
    "        date_sequences.append(date_seq)\n",
    "\n",
    "    return torch.tensor(np.stack(data_sequences)), date_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf7d82-4761-4d4f-b96a-48a76f3b6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sequences(train_dfs, test_dfs, seq_len=120):\n",
    "    \"\"\"\n",
    "    train_dfs(List): Input\n",
    "    test_dfs(List): Input\n",
    "    return train_seq(arr), val_seq(arr): train/val sliding window sequences tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    train_seqs = []\n",
    "    test_seqs = []\n",
    "    train_dates = []\n",
    "    test_dates = []\n",
    "\n",
    "    for train, test in zip(train_dfs, test_dfs):\n",
    "        train_seq, train_date = create_sequences(train, seq_len=seq_len)\n",
    "        test_seq, test_date = create_sequences(test, seq_len=seq_len)\n",
    "        train_seqs.append(train_seq)\n",
    "        train_dates.extend(train_date)\n",
    "        test_seqs.append(test_seq)\n",
    "        test_dates.extend(test_date)\n",
    "    \n",
    "    return torch.cat(train_seqs, dim=0), torch.cat(test_seqs, dim=0), train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14907a24-b18a-4f21-ac1b-ab5b391dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_seq, val_seq, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create the dataloader.\n",
    "    train_seq(tensor)\n",
    "    val_seq(tensor)\n",
    "    return train_loader, val_loader\n",
    "    \"\"\"\n",
    "    # Make TensorDataset ìƒì„± (Same input and output)\n",
    "    train_dataset = TensorDataset(train_seq, train_seq) # X, y\n",
    "    val_dataset = TensorDataset(val_seq, train_seq) # X, y\n",
    "\n",
    "    # Make DataLoader (Load X,y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b986b-5e4f-4c16-9068-0aa72049a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_usad_lstm(model, train_loader, device='cuda', num_epochs=30, sav_path = '', alpha=0.5, beta=0.5, lr = 1e-4):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Initialize the model with device (CPU, GPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Use the ADAM optimizer\n",
    "    optimizer1 = optim.Adam(list(model.encoder.parameters()) + list(model.decoder1.parameters()), lr=lr)\n",
    "    optimizer2 = optim.Adam(list(model.encoder.parameters()) + list(model.decoder2.parameters()), lr=lr)\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')  # ê°€ì¥ ì‘ì€ loss ì¶”ì ìš©\n",
    "    save_path = sav_path  # ì €ì¥í•  í´ë”\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    loss1_list = []\n",
    "    loss2_list = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss1, epoch_loss2 = 0.0, 0.0\n",
    "\n",
    "        for x, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "            x = x.to(device)\n",
    "            # Phase 1\n",
    "            # Initialize the gradient\n",
    "            optimizer1.zero_grad()\n",
    "            # Ouput w1 (_ : w2, z)\n",
    "            w1, _, _ = model(x)\n",
    "            # Calculate Loss function 1\n",
    "            loss1 = alpha * criterion(w1, x)\n",
    "            # Back-propagation\n",
    "            loss1.backward()\n",
    "            # Parameter update with the gradient\n",
    "            optimizer1.step()\n",
    "            epoch_loss1 += loss1.item()\n",
    "\n",
    "            # Phase 2\n",
    "            optimizer2.zero_grad()\n",
    "            # Ouput w1 (_ : w1, z)\n",
    "            _, w2, _ = model(x)\n",
    "            # Calculate Loss function 2\n",
    "            loss2 = beta * criterion(w2, x)\n",
    "            # Back-propagation\n",
    "            loss2.backward()\n",
    "            # Parameter update with the gradient\n",
    "            optimizer2.step()\n",
    "            epoch_loss2 += loss2.item()\n",
    "\n",
    "        avg_loss1 = epoch_loss1 / len(train_loader)\n",
    "        avg_loss2 = epoch_loss2 / len(train_loader)\n",
    "\n",
    "        loss1_list.append(avg_loss1)\n",
    "        loss2_list.append(avg_loss2)\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}] Train Loss1: {epoch_loss1:.6f} | Train Loss2: {epoch_loss2:.6f}\")\n",
    "\n",
    "        # save weights file\n",
    "        if avg_loss1 < best_loss:\n",
    "            best_loss = avg_loss1\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'USAD_best.pth'))\n",
    "            print(f\"[Epoch {epoch}] ëª¨ë¸ ì €ì¥ë¨: val_loss = {avg_loss1:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss1_list, label='Loss1 (Encoder+Decoder1)')\n",
    "    plt.plot(loss2_list, label='Loss2 (Encoder+Decoder2)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sav_path, f'USAD_{seq_len}_train_loss_plot.png'))\n",
    "    plt.close()\n",
    "    print(f\"[Saved] Training loss plot saved at {os.path.join(sav_path, f'USAD_{seq_len}_train_loss_plot.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5fb15-ba0b-4fe7-8668-99c9968fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_usad_lstm(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    total_loss1, total_loss2 = 0.0, 0.0\n",
    "    all_x, all_w2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            w1, w2, _ = model(x)\n",
    "\n",
    "            loss1 = criterion(w1, x).item()\n",
    "            loss2 = criterion(w2, x).item()\n",
    "            total_loss1 += loss1\n",
    "            total_loss2 += loss2\n",
    "\n",
    "            all_x.append(x.cpu())\n",
    "            all_w2.append(w2.cpu())\n",
    "\n",
    "    print(f\"[Test] Loss1: {total_loss1:.6f} | Loss2: {total_loss2:.6f}\")\n",
    "    \n",
    "    # ì‹œê³„ì—´ ì „ì²´ ë³´ì • ê²°ê³¼ ë°˜í™˜\n",
    "    all_x = torch.cat(all_x, dim=0)    # [N, 60, 3]\n",
    "    all_w2 = torch.cat(all_w2, dim=0)  # [N, 60, 3]\n",
    "    return all_x, all_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd3eed-9b81-4988-90a2-12735c207b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(x_seq, w2_seq, mode='vl'):\n",
    "    \"\"\"\n",
    "    x_seq, w2_seq: [N, T, 3]\n",
    "    mode: 'vl' â†’ ê´€ë¡œìˆ˜ìœ„ë§Œ ì‚¬ìš© / 'all' â†’ ì „ì²´ ë³€ìˆ˜ í‰ê· \n",
    "    ë°˜í™˜: [N] shapeì˜ ì´ìƒì¹˜ ì ìˆ˜ ë²¡í„°\n",
    "    \"\"\"\n",
    "    # ê´€ë¡œìˆ˜ìœ„ (index 2)ë§Œ ë¹„êµ\n",
    "    errors = (x_seq - w2_seq) ** 2  # [N, T]\n",
    "    scores = errors.mean(dim=1)  # ì‹œí€€ìŠ¤ë³„ í‰ê·  MSE â†’ [N]\n",
    "    return errors.numpy(), scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1bf05-ee13-41e6-b75f-b026fb9263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(scores, method='iqr', k=3):\n",
    "    \"\"\"\n",
    "    scores: ì´ìƒì¹˜ ì ìˆ˜ ë²¡í„° (numpy)\n",
    "    method: 'iqr', 'mean_std', 'percentile'\n",
    "    \"\"\"\n",
    "    if method == 'iqr':\n",
    "        q1 = np.percentile(scores, 25)\n",
    "        q3 = np.percentile(scores, 75)\n",
    "        iqr = q3 - q1\n",
    "        threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    elif method == 'mean_std':\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        threshold = mean + k * std\n",
    "\n",
    "    elif method == 'percentile':\n",
    "        threshold = np.percentile(scores, 95)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05f9f5-cc0f-4937-ac59-596c0e9b92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corrected_vs_original(original_seq, corrected_seq, input_seq, sav_path = '', sample_idx=0):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ì‹œí€€ìŠ¤ ë¹„êµ ì‹œê°í™” (60ë¶„ê°„)\n",
    "    \"\"\"\n",
    "    orig = original_seq[sample_idx]#.numpy()  # ê´€ë¡œ ìˆ˜ìœ„\n",
    "    corr = corrected_seq[sample_idx]#.numpy()\n",
    "    inputs = input_seq[sample_idx]\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(orig, label='Original (vl)', marker='o')\n",
    "    plt.plot(corr, label='Corrected (w2)', marker='x')\n",
    "    plt.plot(inputs, label='Input', marker='+')\n",
    "    plt.title(f'Sample {sample_idx}: Sewer Level Original  vs Corrected')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'{sav_path}/{sample_idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6523c24-aac7-4956-9545-829d997c735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_on_timeseries(original_seq, anomaly_flags, sav_path='', sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plot(scatter) the anomlay detection results on the graph\n",
    "    original_seq : [N, T] or [N, T, 3]\n",
    "    anomaly_flags : [N, T] (bool array)\n",
    "    sample_idx(int)\n",
    "    \"\"\"\n",
    "    vl = original_seq[sample_idx]  # shape: [T]\n",
    "    flags = anomaly_flags[sample_idx]  # shape: [T], bool\n",
    "\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(vl, label='Sewer Level (vl)', color='blue')\n",
    "\n",
    "    # Scatter the anomaly\n",
    "    if flags.any():\n",
    "        plt.scatter(np.where(flags)[0], vl[flags], color='red', label='Detected Anomaly', zorder=3)\n",
    "\n",
    "    plt.title(f'Sample {sample_idx} - Detected {flags.sum()} Anomalies')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{sav_path}/{sample_idx}_anomaly.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f1d9e-fb8f-416b-9744-c024dc85223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pointwise_anomaly_statistics(errors, anomaly_flags):\n",
    "    \"\"\"\n",
    "    ì‹œì  ë‹¨ìœ„ ì´ìƒì¹˜ íƒì§€ í†µê³„ ìš”ì•½\n",
    "    errors: np.ndarray, shape [N] or [T] - pointwise ì˜¤ì°¨ ë²¡í„° (ì˜ˆ: MAE or MSE)\n",
    "    anomaly_flags: np.ndarray, shape [N] or [T] - ì´ìƒì¹˜ ì—¬ë¶€ (bool ë°°ì—´)\n",
    "\n",
    "    ì¶œë ¥:\n",
    "        - ì´ ì‹œì  ìˆ˜\n",
    "        - ì´ìƒì¹˜ ì‹œì  ìˆ˜ ë° ë¹„ìœ¨\n",
    "        - ì´ìƒì¹˜ í‰ê·  ì˜¤ì°¨\n",
    "        - ì •ìƒê°’ í‰ê·  ì˜¤ì°¨\n",
    "    \"\"\"\n",
    "    total = len(errors)\n",
    "    num_anomalies = np.sum(anomaly_flags)\n",
    "    num_normals = total - num_anomalies\n",
    "\n",
    "    mean_anomaly_error = np.mean(errors[anomaly_flags]) if num_anomalies > 0 else 0.0\n",
    "    mean_normal_error = np.mean(errors[~anomaly_flags]) if num_normals > 0 else 0.0\n",
    "\n",
    "    print(\"ğŸ“Š ì‹œì  ë‹¨ìœ„ ì´ìƒì¹˜ íƒì§€ í†µê³„ ìš”ì•½\")\n",
    "    print(f\"- ì´ ì‹œì  ìˆ˜: {total}\")\n",
    "    print(f\"- ì´ìƒì¹˜ ì‹œì  ìˆ˜: {num_anomalies}\")\n",
    "    print(f\"- ì´ìƒì¹˜ ë¹„ìœ¨: {100.0 * num_anomalies / total:.2f}%\")\n",
    "    print(f\"- ì´ìƒì¹˜ í‰ê·  ì˜¤ì°¨: {mean_anomaly_error:.4f}\")\n",
    "    print(f\"- ì •ìƒê°’ í‰ê·  ì˜¤ì°¨: {mean_normal_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67c5c9-edaf-40ed-b7df-e6bf3835ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_series_from_sequences(seq_array):\n",
    "    \"\"\"\n",
    "    Merge the predicted sequences for all time step data\n",
    "    seq_array: [N, T] numpy or tensor\n",
    "    ë°˜í™˜: [N+T-1] ë°°ì—´ (ì‹œì  ë‹¨ìœ„ ê²°ê³¼)\n",
    "    \"\"\"\n",
    "    # seq_array = seq_array.numpy()\n",
    "    N, T = seq_array.shape\n",
    "    summed = np.zeros(N + T - 1)\n",
    "    count = np.zeros(N + T - 1)\n",
    "\n",
    "    for i in range(N):\n",
    "        summed[i:i+T] += seq_array[i]\n",
    "        count[i:i+T] += 1\n",
    "\n",
    "    return summed / count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a936813-8a0a-4407-acd1-7b7059763365",
   "metadata": {},
   "source": [
    "## Load dataset (Several dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760611e5-da5f-4f86-8bc3-4d4131958f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {0 : 'gwangjoo', 1 : 'changwon', 2 : 'pohang'}\n",
    "fname = {0 : '2920010001045020', 1 : '4812110001018020', 2 : ''}\n",
    "r_cd = 0\n",
    "\n",
    "batch_size = 256\n",
    "seq_len = 240\n",
    "input_dim=3\n",
    "hidden_dim=24\n",
    "latent_dim=16\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "data_dir = './trainset/sewer/'\n",
    "# íŒŒì¼ ê²½ë¡œ ì •ë ¬\n",
    "train_paths = sorted(glob.glob(os.path.join(data_dir, 'train*.csv')))\n",
    "test_paths = sorted(glob.glob(os.path.join(data_dir, 'org*.csv')))\n",
    "\n",
    "# 'flag' ì—´ ì œì™¸í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "usecols = ['dates', 'rn', 'vl', 'wl']  # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ëª…ì‹œ\n",
    "\n",
    "train_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in train_paths]\n",
    "test_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in test_paths]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fafa36-4f41-428a-95a6-bdd812b9536a",
   "metadata": {},
   "source": [
    "## Check NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b7c41-556f-4fa4-ac9c-91345d33faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ì»¬ëŸ¼ë³„ NaN ê°œìˆ˜ ëˆ„ì ìš© (Series ì´ˆê¸°í™”)\n",
    "train_null = pd.Series(0, index=train_dfs[0].columns)\n",
    "test_null = pd.Series(0, index=test_dfs[0].columns)\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    train_null += train.isnull().sum()\n",
    "    test_null += test.isnull().sum()\n",
    "\n",
    "print('Train set NaN ê°œìˆ˜ (ì»¬ëŸ¼ë³„):\\n', train_null)\n",
    "print('Test set NaN ê°œìˆ˜ (ì»¬ëŸ¼ë³„):\\n', test_null)\n",
    "\n",
    "# ì „ì²´ NaN ì´í•© ì¶œë ¥\n",
    "print(f\"\\nTrain set ì „ì²´ NaN ìˆ˜: {train_null.sum()}\")\n",
    "print(f\"Test set ì „ì²´ NaN ìˆ˜: {test_null.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ce317-b821-4057-946f-a8623b4b68fe",
   "metadata": {},
   "source": [
    "## Data normalization (rain value, sewer level(vl), river level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdffd2-f293-4701-8cfd-ade76164e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm_dfs = []\n",
    "test_norm_dfs = []\n",
    "_, train_scaler = data_norm(pd.concat(train_dfs))\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    test.fillna(0.0, inplace=True)\n",
    "    \n",
    "    train_norm, _ = data_norm(train, scaler = train_scaler)\n",
    "    test_norm, _ = data_norm(test, mode='test',scaler=train_scaler)\n",
    "    \n",
    "    test_norm['vl'] = test_norm['vl'].clip(lower= 0.0)\n",
    "    \n",
    "    train_norm_dfs.append(train_norm)\n",
    "    test_norm_dfs.append(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76977f1d-9cc4-40c0-9436-4f78070a936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('â–¶ Train set')\n",
    "tmp_orig = pd.concat(train_dfs)\n",
    "tmp_norm = pd.concat(train_norm_dfs)\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_orig['rn'].min(), tmp_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_norm['rn'].min(), tmp_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_orig['vl'].min(), tmp_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_norm['vl'].min(), tmp_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_orig['wl'].min(), tmp_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_norm['wl'].min(), tmp_norm['wl'].max())\n",
    "\n",
    "print('\\nâ–¶ Test set')\n",
    "tmp_test_orig = pd.concat(test_dfs)\n",
    "tmp_test_norm = pd.concat(test_norm_dfs)\n",
    "\n",
    "print('Test set Nan count:\\n', tmp_test_orig.isnull().sum())\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_test_orig['rn'].min(), tmp_test_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_test_norm['rn'].min(), tmp_test_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_test_orig['vl'].min(), tmp_test_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_test_norm['vl'].min(), tmp_test_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_test_orig['wl'].min(), tmp_test_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_test_norm['wl'].min(), tmp_test_norm['wl'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e8e91-42f8-463b-adab-c57e540f031d",
   "metadata": {},
   "source": [
    "## Make dataset for the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13272cf-2a49-42bd-aec0-d8e976ada5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, val_seq, train_date, test_date = create_combined_sequences(train_norm_dfs, test_norm_dfs, seq_len=seq_len)\n",
    "\n",
    "print(\"Train shape:\", train_seq.shape)  # (N_train, 60, 3)\n",
    "print(\"Val shape:\", val_seq.shape)      # (N_val, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f643a0-e260-4802-bde2-75c355b44c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = np.stack(train_date)\n",
    "test_date = np.stack(test_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02117f-6d62-4b16-a046-71215ed8a028",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617089c-0cc4-4388-9ab2-4fd2dc715194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = USAD_LSTM(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim, seq_len=seq_len)\n",
    "example = torch.randn((batch_size,seq_len, input_dim)) \n",
    "w1, w2, z = model(example)\n",
    "\n",
    "print(w1.shape)  # torch.Size([32, 10, 3])\n",
    "print(w2.shape)  # torch.Size([32, 10, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ecfa7-d608-4c0d-9ea1-13d0d18a0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(example.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d2d21-2646-40ae-b264-cb725bcbe6f7",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ee97a-efb1-44ec-b6c6-7ddc2f9325e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loader to load the dataset\n",
    "train_loader, val_loader = create_dataloaders(train_seq, val_seq, batch_size=batch_size)\n",
    "\n",
    "# Check the dataset shape\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)  \n",
    "    print(y.shape)  \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953fe60-d801-48df-815b-d506472d84f9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12615f56-be0c-4a02-832a-49122f13838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Start\n",
    "train_usad_lstm(model, train_loader, device=device, num_epochs=epochs,sav_path=f'./sav/USAD_{seq_len}_rn_vl_wl_gc/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c285b7e-4dda-4a1e-b298-aec09c3b1737",
   "metadata": {},
   "source": [
    "## Load the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6459e-2306-4ca7-8bbf-133cd4034f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./sav/USAD_{seq_len}_rn_vl_wl_gc/USAD_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db887e5-131e-4d57-a680-d2c76830974d",
   "metadata": {},
   "source": [
    "## Predict and get the predicted sewer level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fb1fc-b906-46c0-948e-7a9bfc355ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "original_seq, corrected_seq = test_usad_lstm(model, val_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44366d-9458-4c33-a009-16e6862d5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_seq = inverse_norm(original_seq, train_scaler)\n",
    "corrected_seq = inverse_norm(corrected_seq, train_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbaa0e8-6a3b-4ac8-af0d-ffed1a6a3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_value_g = test_dfs[0]['rn']\n",
    "rain_value_c = test_dfs[1]['rn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a962ea-6e15-43da-97e5-f25b57ed1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the correted sequence only sewer level\"\n",
    "original_vl =  original_seq[:, :, 1]  # [N, 60]\n",
    "corrected_vl = corrected_seq[:, :, 1]  # [N, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665fa49-6612-4a46-9994-a74498632891",
   "metadata": {},
   "source": [
    "## Merge the sequences data to the all time step data ([N,T] -> [N+T-1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbc83e-2542-4daf-abc0-b055580460a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = int(len(original_seq)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e575ba-a9bf-48fc-8793-5c114f9a117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ì „ì²´ ì‹œê³„ì—´ ë³µì›\n",
    "original_series_g = reconstruct_series_from_sequences(original_vl[:md])\n",
    "corrected_series_g = reconstruct_series_from_sequences(corrected_vl[:md])\n",
    "val_series_g = np.stack(test_dfs[0]['vl'].to_list())\n",
    "original_series_c = reconstruct_series_from_sequences(original_vl[md:])\n",
    "corrected_series_c = reconstruct_series_from_sequences(corrected_vl[md:])\n",
    "val_series_c = np.stack(test_dfs[1]['vl'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160710f7-9a74-4cae-89ab-8cb08faaf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_errors_g = (corrected_series_g - val_series_g) ** 2  # [N + T - 1]\n",
    "point_errors_c = (corrected_series_c - val_series_c) ** 2  # [N + T - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e2fae-3f88-4d67-a21b-57f1937cc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.date_range('20240301','20241201',freq='T',inclusive='left')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd6d7-fc2d-4220-86ea-bb1f773a55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(scores, method='percentile', percentile=98):\n",
    "    return np.percentile(scores, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84d1f8-8110-4ef4-930a-0195db34d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_g = determine_threshold(point_errors_g, method='iqr')  # ë˜ëŠ” 'mean_std'\n",
    "# threshold_c = determine_threshold(point_errors_c, method='iqr')  # ë˜ëŠ” 'mean_std'\n",
    "threshold_g = determine_threshold(point_errors_g)  # ë˜ëŠ” 'mean_std'\n",
    "threshold_c = determine_threshold(point_errors_c)  # ë˜ëŠ” 'mean_std'\n",
    "threshold_g = max(threshold_g, 4.0)\n",
    "threshold_c = max(threshold_c, 4.0)\n",
    "anomaly_flags_g = point_errors_g > threshold_g  # [N + T - 1] í˜•íƒœ\n",
    "anomaly_flags_c = point_errors_c > threshold_c  # [N + T - 1] í˜•íƒœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5518d03-9d67-430f-b0cd-2754c68e1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ê´‘ì£¼ ì„ê³„ê°’:\", threshold_g)\n",
    "print(\"ê´‘ì£¼ íƒì§€ëœ ì´ìƒì¹˜ ìˆ˜:\", np.sum(anomaly_flags_g))\n",
    "print(\"ì°½ì› ì„ê³„ê°’:\", threshold_c)\n",
    "print(\"ì°½ì› íƒì§€ëœ ì´ìƒì¹˜ ìˆ˜:\", np.sum(anomaly_flags_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d1e77-f2ef-4def-b592-e949b5276eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_norm_g = original_series_g.round()\n",
    "corrected_norm_g = corrected_series_g.round()\n",
    "input_seq_g = val_series_g.round()\n",
    "original_norm_c = original_series_c.round()\n",
    "corrected_norm_c = corrected_series_c.round()\n",
    "input_seq_c = val_series_c.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdddb11-1868-42ee-94b5-c0ca0296335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decadal_timeseries_with_anomalies(\n",
    "    tt, original, predicted, input_data=None, anomaly_flags=None,\n",
    "    save_dir=None, interval_days=10, rain=None\n",
    "):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ì„ 10ì¼ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì‹œê°í™”í•˜ë©°, ì´ìƒì¹˜ëŠ” ë¹¨ê°„ ì ìœ¼ë¡œ í‘œì‹œí•˜ê³ \n",
    "    ê°•ìˆ˜ëŸ‰(rain)ì€ íšŒìƒ‰ ì‹¤ì„ ìœ¼ë¡œ ìš°ì¸¡ yì¶•ì— ì‹œê°í™”\n",
    "\n",
    "    Parameters:\n",
    "    - tt: DatetimeIndex, ì‹œê³„ì—´ ì‹œê°„\n",
    "    - original: np.array [N]\n",
    "    - predicted: np.array [N]\n",
    "    - input_data: np.array [N] or None\n",
    "    - anomaly_flags: np.array(bool) [N] or None\n",
    "    - save_dir: ì €ì¥ í´ë” ê²½ë¡œ\n",
    "    - interval_days: ë©€í‹° í”Œë¡¯ ë‹¨ìœ„ ê¸°ê°„ (ê¸°ë³¸ 10ì¼)\n",
    "    - rain: ê°•ìˆ˜ëŸ‰ np.array [N] or None\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'date': tt,\n",
    "        'original': original,\n",
    "        'predicted': predicted\n",
    "    })\n",
    "    if input_data is not None:\n",
    "        df['input'] = input_data\n",
    "    if anomaly_flags is not None:\n",
    "        df['anomaly'] = anomaly_flags\n",
    "    else:\n",
    "        df['anomaly'] = False\n",
    "    if rain is not None:\n",
    "        df['rain'] = rain\n",
    "\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # ë‚ ì§œ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    periods = pd.date_range(start=start_date, end=end_date, freq=f'{interval_days}D')\n",
    "\n",
    "    for i in range(len(periods) - 1):\n",
    "        t0, t1 = periods[i], periods[i+1]\n",
    "        window = df[(df.index >= t0) & (df.index < t1)]\n",
    "\n",
    "        if window.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "        ax1.plot(window.index, window['original'], label='Original', color='black')\n",
    "        ax1.plot(window.index, window['predicted'], label='Reconstructed', color='green', alpha=0.7)\n",
    "        if 'input' in window.columns:\n",
    "            ax1.plot(window.index, window['input'], label='Input', color='blue', alpha=0.5)\n",
    "\n",
    "        # ì´ìƒì¹˜ ì‹œê°í™”\n",
    "        anomaly_points = window[window['anomaly']]\n",
    "        if not anomaly_points.empty:\n",
    "            ax1.scatter(\n",
    "                anomaly_points.index,\n",
    "                anomaly_points['original'],\n",
    "                color='red',\n",
    "                label='Anomaly',\n",
    "                zorder=3\n",
    "            )\n",
    "\n",
    "        # ìš°ì¸¡ yì¶•: ê°•ìˆ˜ëŸ‰ (íšŒìƒ‰ ì‹¤ì„ )\n",
    "        if 'rain' in window.columns:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(window.index, window['rain'], label='Rainfall', color='gray', linestyle='-', alpha=0.6)\n",
    "            ax2.set_ylabel('Rainfall (mm)', color='gray')\n",
    "            ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "        ax1.set_title(f'{t0.date()} ~ {t1.date()} Sewer Level Reconstruction')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Sewer Level')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend(loc='upper left')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            fname = f\"{t0.strftime('%Y%m%d')}_{t1.strftime('%Y%m%d')}.png\"\n",
    "            plt.savefig(os.path.join(save_dir, fname), dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"âœ… Saved: {fname}\")\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc7ddc-c4b4-43b5-9254-bf026909de13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "plot_decadal_timeseries_with_anomalies(\n",
    "    tt=tt,\n",
    "    original=original_norm_g, \n",
    "    predicted=corrected_norm_g, \n",
    "    input_data=input_seq_g,\n",
    "    anomaly_flags=anomaly_flags_g,  \n",
    "    save_dir=f'./imgs/USAD_{seq_len}_gc/gwangjoo/',\n",
    "    interval_days=5,\n",
    "    rain = rain_value_g\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af240ef5-0a38-4bd8-8410-88a42b9d779d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "plot_decadal_timeseries_with_anomalies(\n",
    "    tt=tt,\n",
    "    original=original_norm_c, \n",
    "    predicted=corrected_norm_c, \n",
    "    input_data=input_seq_c,\n",
    "    anomaly_flags=anomaly_flags_c,  \n",
    "    save_dir=f'./imgs/USAD_{seq_len}_gc/changwon/',\n",
    "    interval_days=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67821b-d101-40ef-b691-8c03e8768fcb",
   "metadata": {},
   "source": [
    "## Gwangjoo anamaly statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565335a-63a8-4cf3-9319-8fdd7896d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_pointwise_anomaly_statistics(point_errors_g, anomaly_flags_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff82a2-2d57-4265-b652-11593b35622a",
   "metadata": {},
   "source": [
    "## Changwon anamaly statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b8a3f-dd55-4406-abf5-52993fb07c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_pointwise_anomaly_statistics(point_errors_c, anomaly_flags_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616131-c1e4-4cf6-81bb-484369cf84bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
