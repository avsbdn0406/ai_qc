{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3983f-a7ad-4eb9-83c9-1c34cb9b9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c45caf-46c4-4153-8083-9ccea86cb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57720ead-73fd-410a-83db-b2c3b0203b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6f2bf-d6f3-4836-be76-562ee67f5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, latent_size=16):\n",
    "        super(SeqEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, F]\n",
    "        _, (h_n, _) = self.lstm(x)  # h_n: [1, B, H]\n",
    "        z = self.fc(h_n.squeeze(0))  # z: [B, latent_size]\n",
    "        return z\n",
    "\n",
    "class SeqDecoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size=64, output_size=3, seq_len=120):\n",
    "        super(SeqDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.fc = nn.Linear(latent_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, output_size, batch_first=True)\n",
    "\n",
    "    def forward(self, z):  # z: [B, latent_size]\n",
    "        h0 = self.fc(z).unsqueeze(0)  # [1, B, H]\n",
    "        h_seq = h0.repeat(self.seq_len, 1, 1).permute(1, 0, 2)  # [B, T, H]\n",
    "        out, _ = self.lstm(h_seq)  # [B, T, F]\n",
    "        return out\n",
    "\n",
    "class TranAD(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=64, latent_size=16, seq_len=120):\n",
    "        super(TranAD, self).__init__()\n",
    "        self.encoder = SeqEncoder(input_size, hidden_size, latent_size)\n",
    "        self.decoder = SeqDecoder(latent_size, hidden_size, input_size, seq_len)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, F]\n",
    "        z = self.encoder(x)             # [B, latent_size]\n",
    "        x_hat = self.decoder(z)         # [B, T, F]\n",
    "        z_hat = self.encoder(x_hat)     # [B, latent_size]\n",
    "        return x_hat, z.unsqueeze(1).repeat(1, self.seq_len, 1), z_hat.unsqueeze(1).repeat(1, self.seq_len, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac2cc9-dc80-499a-a8b4-508c404f0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm(df,mode='train',scaler=''):\n",
    "    \"\"\"\n",
    "    Normalize the data.\n",
    "    df(dataframe) : Input\n",
    "    return tmp(dataframe), scaler : normalized dataframe\n",
    "    \"\"\"\n",
    "    columns = df.columns[1:]\n",
    "\n",
    "    tmp = df.copy()\n",
    "    if mode == 'train':\n",
    "        # Normalize\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(tmp[columns])\n",
    "    elif mode=='test':\n",
    "        scaler = scaler\n",
    "        scaled = scaler.transform(tmp[columns])\n",
    "    # Insert the normalized value to the original frame\n",
    "    tmp[columns] = scaled\n",
    "\n",
    "    return tmp, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c81c5-7df9-4632-93e3-8a278fd95dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_norm(tensor, scaler):\n",
    "    \"\"\"\n",
    "    tensor: [N, T, C] → numpy 배열로 변환 후 역정규화 수행\n",
    "    \"\"\"\n",
    "    shape = tensor.shape\n",
    "    data = tensor.reshape(-1, shape[-1])\n",
    "    inv = scaler.inverse_transform(data)\n",
    "    return inv.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a768ac1-c09f-4aba-8acb-ff87022ded33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_len=10):\n",
    "    \"\"\"\n",
    "    날짜 포함된 시퀀스 생성 함수\n",
    "    df: ['date', 'rn', 'vl', 'wl'] 포함된 DataFrame\n",
    "    반환: (data_seq, date_seq)\n",
    "        - data_seq: torch.Tensor [N, seq_len, 3]\n",
    "        - date_seq: List[List[str]] [N, seq_len]\n",
    "    \"\"\"\n",
    "    cols = df.columns[1:]  # 날짜 제외한 나머지 변수\n",
    "    values = df[cols].values.astype(np.float32)\n",
    "    dates = df['dates'].values  # 문자열 형태로 추출\n",
    "\n",
    "    data_sequences = []\n",
    "    date_sequences = []\n",
    "\n",
    "    for i in range(len(df) - seq_len + 1):\n",
    "        data_seq = values[i:i+seq_len]\n",
    "        date_seq = dates[i:i+seq_len]\n",
    "        data_sequences.append(data_seq)\n",
    "        date_sequences.append(date_seq)\n",
    "\n",
    "    return torch.tensor(np.stack(data_sequences)), date_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf7d82-4761-4d4f-b96a-48a76f3b6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sequences(train_dfs, test_dfs, seq_len=120):\n",
    "    \"\"\"\n",
    "    train_dfs(List): Input\n",
    "    test_dfs(List): Input\n",
    "    return train_seq(arr), val_seq(arr): train/val sliding window sequences tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    train_seqs = []\n",
    "    test_seqs = []\n",
    "    train_dates = []\n",
    "    test_dates = []\n",
    "\n",
    "    for train, test in zip(train_dfs, test_dfs):\n",
    "        train_seq, train_date = create_sequences(train, seq_len=seq_len)\n",
    "        test_seq, test_date = create_sequences(test, seq_len=seq_len)\n",
    "        train_seqs.append(train_seq)\n",
    "        train_dates.extend(train_date)\n",
    "        test_seqs.append(test_seq)\n",
    "        test_dates.extend(test_date)\n",
    "    \n",
    "    return torch.cat(train_seqs, dim=0), torch.cat(test_seqs, dim=0), train_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14907a24-b18a-4f21-ac1b-ab5b391dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_seq, val_seq, batch_size=64, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create the dataloader.\n",
    "    train_seq(tensor)\n",
    "    val_seq(tensor)\n",
    "    return train_loader, val_loader\n",
    "    \"\"\"\n",
    "    # Make TensorDataset 생성 (Same input and output)\n",
    "    train_dataset = TensorDataset(train_seq, train_seq) # X, y\n",
    "    val_dataset = TensorDataset(val_seq, train_seq) # X, y\n",
    "\n",
    "    # Make DataLoader (Load X,y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe3996-8a85-4834-a4a8-c4ce17039cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tranad(model, train_loader, num_epochs=50, alpha=0.5, lr=1e-3, device='cuda',sav_path=None):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')  # 가장 작은 loss 추적용\n",
    "    save_path = sav_path  # 저장할 폴더\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for x, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, z, z_hat = model(x)\n",
    "\n",
    "            loss1 = criterion(x_hat, x)\n",
    "            loss2 = criterion(z_hat, z)\n",
    "            loss = alpha * loss1 + (1 - alpha) * loss2\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}\")\n",
    "        losses.append(total_loss)\n",
    "        # save weights file\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'TranAD_best.pth'))\n",
    "            print(f\"[Epoch {epoch}] 모델 저장됨: val_loss = {total_loss:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(losses, label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sav_path, f'TranAD_{seq_len}_train_loss_plot.png'))\n",
    "    plt.close()\n",
    "    print(f\"[Saved] Training loss plot saved at {os.path.join(sav_path, f'TranAD_{seq_len}_train_loss_plot.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5fb15-ba0b-4fe7-8668-99c9968fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tranad(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    recon_errors = []\n",
    "    all_x = []\n",
    "    all_xhat = []\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='none')  # feature별 loss 확인 위해\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            x_hat, _, _ = model(x)  # z, z_hat은 여기선 사용하지 않음\n",
    "\n",
    "            # reconstruction error 계산\n",
    "            loss = (x_hat - x) ** 2  # [B, T, F]\n",
    "            loss = loss.mean(dim=2)  # feature 평균 → [B, T]\n",
    "            recon_errors.append(loss.cpu().numpy())\n",
    "\n",
    "            # 시계열 저장\n",
    "            all_x.append(x.cpu())\n",
    "            all_xhat.append(x_hat.cpu())\n",
    "\n",
    "    # concat\n",
    "    all_x = torch.cat(all_x, dim=0)         # [N, T, F]\n",
    "    all_xhat = torch.cat(all_xhat, dim=0)   # [N, T, F]\n",
    "    recon_errors = np.concatenate(recon_errors, axis=0)  # [N, T]\n",
    "\n",
    "    return all_x, all_xhat, recon_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd3eed-9b81-4988-90a2-12735c207b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(x_seq, w2_seq, mode='vl'):\n",
    "    \"\"\"\n",
    "    x_seq, w2_seq: [N, T, 3]\n",
    "    mode: 'vl' → 관로수위만 사용 / 'all' → 전체 변수 평균\n",
    "    반환: [N] shape의 이상치 점수 벡터\n",
    "    \"\"\"\n",
    "    # 관로수위 (index 2)만 비교\n",
    "    errors = (x_seq - w2_seq) ** 2  # [N, T]\n",
    "    scores = errors.mean(dim=1)  # 시퀀스별 평균 MSE → [N]\n",
    "    return errors.numpy(), scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1bf05-ee13-41e6-b75f-b026fb9263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(scores, method='iqr', k=3):\n",
    "    \"\"\"\n",
    "    scores: 이상치 점수 벡터 (numpy)\n",
    "    method: 'iqr', 'mean_std', 'percentile'\n",
    "    \"\"\"\n",
    "    if method == 'iqr':\n",
    "        q1 = np.percentile(scores, 25)\n",
    "        q3 = np.percentile(scores, 75)\n",
    "        iqr = q3 - q1\n",
    "        threshold = q3 + 1.5 * iqr\n",
    "\n",
    "    elif method == 'mean_std':\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        threshold = mean + k * std\n",
    "\n",
    "    elif method == 'percentile':\n",
    "        threshold = np.percentile(scores, 95)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 방식입니다.\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05f9f5-cc0f-4937-ac59-596c0e9b92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corrected_vs_original(original_seq, corrected_seq, input_seq, sav_path = '', sample_idx=0):\n",
    "    \"\"\"\n",
    "    단일 시퀀스 비교 시각화 (60분간)\n",
    "    \"\"\"\n",
    "    orig = original_seq[sample_idx]#.numpy()  # 관로 수위\n",
    "    corr = corrected_seq[sample_idx]#.numpy()\n",
    "    inputs = input_seq[sample_idx]\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(orig, label='Original (vl)', marker='o')\n",
    "    plt.plot(corr, label='Corrected (w2)', marker='x')\n",
    "    plt.plot(inputs, label='Input', marker='+')\n",
    "    plt.title(f'Sample {sample_idx}: Sewer Level Original  vs Corrected')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'{sav_path}/{sample_idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6523c24-aac7-4956-9545-829d997c735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_on_timeseries(original_seq, anomaly_flags, sav_path='', sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plot(scatter) the anomlay detection results on the graph\n",
    "    original_seq : [N, T] or [N, T, 3]\n",
    "    anomaly_flags : [N, T] (bool array)\n",
    "    sample_idx(int)\n",
    "    \"\"\"\n",
    "    vl = original_seq[sample_idx]  # shape: [T]\n",
    "    flags = anomaly_flags[sample_idx]  # shape: [T], bool\n",
    "\n",
    "    if not os.path.isdir(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(vl, label='Sewer Level (vl)', color='blue')\n",
    "\n",
    "    # Scatter the anomaly\n",
    "    if flags.any():\n",
    "        plt.scatter(np.where(flags)[0], vl[flags], color='red', label='Detected Anomaly', zorder=3)\n",
    "\n",
    "    plt.title(f'Sample {sample_idx} - Detected {flags.sum()} Anomalies')\n",
    "    plt.xlabel('Time index (minute)')\n",
    "    plt.ylabel('Sewer Level')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{sav_path}/{sample_idx}_anomaly.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f1d9e-fb8f-416b-9744-c024dc85223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pointwise_anomaly_statistics(errors, anomaly_flags):\n",
    "    \"\"\"\n",
    "    시점 단위 이상치 탐지 통계 요약\n",
    "    errors: np.ndarray, shape [N] or [T] - pointwise 오차 벡터 (예: MAE or MSE)\n",
    "    anomaly_flags: np.ndarray, shape [N] or [T] - 이상치 여부 (bool 배열)\n",
    "\n",
    "    출력:\n",
    "        - 총 시점 수\n",
    "        - 이상치 시점 수 및 비율\n",
    "        - 이상치 평균 오차\n",
    "        - 정상값 평균 오차\n",
    "    \"\"\"\n",
    "    total = len(errors)\n",
    "    num_anomalies = np.sum(anomaly_flags)\n",
    "    num_normals = total - num_anomalies\n",
    "\n",
    "    mean_anomaly_error = np.mean(errors[anomaly_flags]) if num_anomalies > 0 else 0.0\n",
    "    mean_normal_error = np.mean(errors[~anomaly_flags]) if num_normals > 0 else 0.0\n",
    "\n",
    "    print(\"📊 시점 단위 이상치 탐지 통계 요약\")\n",
    "    print(f\"- 총 시점 수: {total}\")\n",
    "    print(f\"- 이상치 시점 수: {num_anomalies}\")\n",
    "    print(f\"- 이상치 비율: {100.0 * num_anomalies / total:.2f}%\")\n",
    "    print(f\"- 이상치 평균 오차: {mean_anomaly_error:.4f}\")\n",
    "    print(f\"- 정상값 평균 오차: {mean_normal_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67c5c9-edaf-40ed-b7df-e6bf3835ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_series_from_sequences(seq_array):\n",
    "    \"\"\"\n",
    "    Merge the predicted sequences for all time step data\n",
    "    seq_array: [N, T] numpy or tensor\n",
    "    반환: [N+T-1] 배열 (시점 단위 결과)\n",
    "    \"\"\"\n",
    "    # seq_array = seq_array.numpy()\n",
    "    N, T = seq_array.shape\n",
    "    summed = np.zeros(N + T - 1)\n",
    "    count = np.zeros(N + T - 1)\n",
    "\n",
    "    for i in range(N):\n",
    "        summed[i:i+T] += seq_array[i]\n",
    "        count[i:i+T] += 1\n",
    "\n",
    "    return summed / count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a936813-8a0a-4407-acd1-7b7059763365",
   "metadata": {},
   "source": [
    "## Load dataset (Several dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760611e5-da5f-4f86-8bc3-4d4131958f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {0 : 'gwangjoo', 1 : 'changwon', 2 : 'pohang'}\n",
    "fname = {0 : '2920010001045020', 1 : '4812110001018020', 2 : ''}\n",
    "r_cd = 0\n",
    "\n",
    "batch_size = 256\n",
    "seq_len = 240\n",
    "input_dim=3\n",
    "hidden_dim=24\n",
    "latent_dim=16\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "data_dir = './trainset/sewer/'\n",
    "# 파일 경로 정렬\n",
    "train_paths = sorted(glob.glob(os.path.join(data_dir, 'train*.csv')))\n",
    "test_paths = sorted(glob.glob(os.path.join(data_dir, 'org*.csv')))\n",
    "\n",
    "# 'flag' 열 제외하고 불러오기\n",
    "usecols = ['dates', 'rn', 'vl', 'wl']  # 필요한 컬럼만 명시\n",
    "\n",
    "train_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in train_paths]\n",
    "test_dfs = [pd.read_csv(path, encoding='cp949', usecols=usecols) for path in test_paths]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fafa36-4f41-428a-95a6-bdd812b9536a",
   "metadata": {},
   "source": [
    "## Check NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b7c41-556f-4fa4-ac9c-91345d33faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 컬럼별 NaN 개수 누적용 (Series 초기화)\n",
    "train_null = pd.Series(0, index=train_dfs[0].columns)\n",
    "test_null = pd.Series(0, index=test_dfs[0].columns)\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    train_null += train.isnull().sum()\n",
    "    test_null += test.isnull().sum()\n",
    "\n",
    "print('Train set NaN 개수 (컬럼별):\\n', train_null)\n",
    "print('Test set NaN 개수 (컬럼별):\\n', test_null)\n",
    "\n",
    "# 전체 NaN 총합 출력\n",
    "print(f\"\\nTrain set 전체 NaN 수: {train_null.sum()}\")\n",
    "print(f\"Test set 전체 NaN 수: {test_null.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ce317-b821-4057-946f-a8623b4b68fe",
   "metadata": {},
   "source": [
    "## Data normalization (rain value, sewer level(vl), river level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdffd2-f293-4701-8cfd-ade76164e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm_dfs = []\n",
    "test_norm_dfs = []\n",
    "_, train_scaler = data_norm(pd.concat(train_dfs))\n",
    "\n",
    "for train, test in zip(train_dfs, test_dfs):\n",
    "    test.fillna(0.0, inplace=True)\n",
    "    \n",
    "    train_norm, _ = data_norm(train, scaler = train_scaler)\n",
    "    test_norm, _ = data_norm(test, mode='test',scaler=train_scaler)\n",
    "    \n",
    "    test_norm['vl'] = test_norm['vl'].clip(lower= 0.0)\n",
    "    \n",
    "    train_norm_dfs.append(train_norm)\n",
    "    test_norm_dfs.append(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76977f1d-9cc4-40c0-9436-4f78070a936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('▶ Train set')\n",
    "tmp_orig = pd.concat(train_dfs)\n",
    "tmp_norm = pd.concat(train_norm_dfs)\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_orig['rn'].min(), tmp_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_norm['rn'].min(), tmp_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_orig['vl'].min(), tmp_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_norm['vl'].min(), tmp_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_orig['wl'].min(), tmp_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_norm['wl'].min(), tmp_norm['wl'].max())\n",
    "\n",
    "print('\\n▶ Test set')\n",
    "tmp_test_orig = pd.concat(test_dfs)\n",
    "tmp_test_norm = pd.concat(test_norm_dfs)\n",
    "\n",
    "print('Test set Nan count:\\n', tmp_test_orig.isnull().sum())\n",
    "\n",
    "print('Rain (rn) - Original min/max:', tmp_test_orig['rn'].min(), tmp_test_orig['rn'].max())\n",
    "print('Rain (rn) - Normalized min/max:', tmp_test_norm['rn'].min(), tmp_test_norm['rn'].max())\n",
    "\n",
    "print('Sewer level (vl) - Original min/max:', tmp_test_orig['vl'].min(), tmp_test_orig['vl'].max())\n",
    "print('Sewer level (vl) - Normalized min/max:', tmp_test_norm['vl'].min(), tmp_test_norm['vl'].max())\n",
    "\n",
    "print('River level (wl) - Original min/max:', tmp_test_orig['wl'].min(), tmp_test_orig['wl'].max())\n",
    "print('River level (wl) - Normalized min/max:', tmp_test_norm['wl'].min(), tmp_test_norm['wl'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e8e91-42f8-463b-adab-c57e540f031d",
   "metadata": {},
   "source": [
    "## Make dataset for the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13272cf-2a49-42bd-aec0-d8e976ada5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, val_seq, train_date, test_date = create_combined_sequences(train_norm_dfs, test_norm_dfs, seq_len=seq_len)\n",
    "\n",
    "print(\"Train shape:\", train_seq.shape)  # (N_train, 60, 3)\n",
    "print(\"Val shape:\", val_seq.shape)      # (N_val, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f643a0-e260-4802-bde2-75c355b44c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = np.stack(train_date)\n",
    "test_date = np.stack(test_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02117f-6d62-4b16-a046-71215ed8a028",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617089c-0cc4-4388-9ab2-4fd2dc715194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranAD(input_size=input_dim, hidden_size=hidden_dim, latent_size=latent_dim, seq_len=seq_len)\n",
    "example = torch.randn((batch_size,seq_len, input_dim)) \n",
    "w1, w2, z = model(example)\n",
    "\n",
    "print(w1.shape)  # torch.Size([32, 10, 3])\n",
    "print(w2.shape)  # torch.Size([32, 10, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ecfa7-d608-4c0d-9ea1-13d0d18a0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(example.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d2d21-2646-40ae-b264-cb725bcbe6f7",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ee97a-efb1-44ec-b6c6-7ddc2f9325e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loader to load the dataset\n",
    "train_loader, val_loader = create_dataloaders(train_seq, val_seq, batch_size=batch_size)\n",
    "\n",
    "# Check the dataset shape\n",
    "for x, y in train_loader:\n",
    "    print(x.shape)  \n",
    "    print(y.shape)  \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953fe60-d801-48df-815b-d506472d84f9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12615f56-be0c-4a02-832a-49122f13838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Start\n",
    "train_tranad(model, train_loader, device=device, num_epochs=epochs,sav_path=f'./sav/TrainAD_{seq_len}_rn_vl_wl_gc/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c285b7e-4dda-4a1e-b298-aec09c3b1737",
   "metadata": {},
   "source": [
    "## Load the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6459e-2306-4ca7-8bbf-133cd4034f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./sav/USAD_{seq_len}_rn_vl_wl_gc/USAD_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db887e5-131e-4d57-a680-d2c76830974d",
   "metadata": {},
   "source": [
    "## Predict and get the predicted sewer level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fb1fc-b906-46c0-948e-7a9bfc355ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "original_seq, corrected_seq, recon_errors = test_tranad(model, val_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44366d-9458-4c33-a009-16e6862d5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_seq = inverse_norm(original_seq, train_scaler)\n",
    "corrected_seq = inverse_norm(corrected_seq, train_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9864381-3637-4940-bd26-7022a444059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_value_g = test_dfs[0]['rn']\n",
    "rain_value_c = test_dfs[1]['rn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a80a9-aae4-4559-8046-41302d2bef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the correted sequence only sewer level\"\n",
    "original_vl =  original_seq[:, :, 1]  # [N, 60]\n",
    "corrected_vl = corrected_seq[:, :, 1]  # [N, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccaef1-366d-4176-a489-cbd467dcd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_errors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665fa49-6612-4a46-9994-a74498632891",
   "metadata": {},
   "source": [
    "## Merge the sequences data to the all time step data ([N,T] -> [N+T-1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbc83e-2542-4daf-abc0-b055580460a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = int(len(original_seq)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e575ba-a9bf-48fc-8793-5c114f9a117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 전체 시계열 복원\n",
    "original_series_g = reconstruct_series_from_sequences(original_vl[:md])\n",
    "corrected_series_g = reconstruct_series_from_sequences(corrected_vl[:md])\n",
    "val_series_g = np.stack(test_dfs[0]['vl'].to_list())\n",
    "original_series_c = reconstruct_series_from_sequences(original_vl[md:])\n",
    "corrected_series_c = reconstruct_series_from_sequences(corrected_vl[md:])\n",
    "val_series_c = np.stack(test_dfs[1]['vl'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160710f7-9a74-4cae-89ab-8cb08faaf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_errors_g = (corrected_series_g - val_series_g) ** 2  # [N + T - 1]\n",
    "point_errors_c = (corrected_series_c - val_series_c) ** 2  # [N + T - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e2fae-3f88-4d67-a21b-57f1937cc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.date_range('20240301','20241201',freq='T',inclusive='left')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd6d7-fc2d-4220-86ea-bb1f773a55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(scores, method='percentile', percentile=98):\n",
    "    return np.percentile(scores, percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84d1f8-8110-4ef4-930a-0195db34d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_g = determine_threshold(point_errors_g, method='iqr')  # 또는 'mean_std'\n",
    "# threshold_c = determine_threshold(point_errors_c, method='iqr')  # 또는 'mean_std'\n",
    "threshold_g = determine_threshold(point_errors_g)  # 또는 'mean_std'\n",
    "threshold_c = determine_threshold(point_errors_c)  # 또는 'mean_std'\n",
    "threshold_g = max(threshold_g, 4.0).round()\n",
    "threshold_c = max(threshold_c, 4.0).round()\n",
    "anomaly_flags_g = point_errors_g > threshold_g  # [N + T - 1] 형태\n",
    "anomaly_flags_c = point_errors_c > threshold_c  # [N + T - 1] 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5518d03-9d67-430f-b0cd-2754c68e1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"광주 임계값:\", threshold_g)\n",
    "print(\"광주 탐지된 이상치 수:\", np.sum(anomaly_flags_g))\n",
    "print(\"창원 임계값:\", threshold_c)\n",
    "print(\"창원 탐지된 이상치 수:\", np.sum(anomaly_flags_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a2b30-0a99-43e1-86d0-6155576766cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"광주 임계값:\", threshold_g)\n",
    "print(\"광주 탐지된 이상치 수:\", np.sum(anomaly_flags_g))\n",
    "print(\"창원 임계값:\", threshold_c)\n",
    "print(\"창원 탐지된 이상치 수:\", np.sum(anomaly_flags_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d1e77-f2ef-4def-b592-e949b5276eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_norm_g = original_series_g.round()\n",
    "corrected_norm_g = corrected_series_g.round()\n",
    "input_seq_g = val_series_g.round()\n",
    "original_norm_c = original_series_c.round()\n",
    "corrected_norm_c = corrected_series_c.round()\n",
    "input_seq_c = val_series_c.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69842e8c-2e5a-4977-9c1f-0fa25c5d73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decadal_timeseries_with_anomalies(\n",
    "    tt, original, predicted, input_data=None, anomaly_flags=None,\n",
    "    save_dir=None, interval_days=10, rain=None\n",
    "):\n",
    "    \"\"\"\n",
    "    시계열을 10일 단위로 나누어 시각화하며, 이상치는 빨간 점으로 표시하고\n",
    "    강수량(rain)은 회색 실선으로 우측 y축에 시각화\n",
    "\n",
    "    Parameters:\n",
    "    - tt: DatetimeIndex, 시계열 시간\n",
    "    - original: np.array [N]\n",
    "    - predicted: np.array [N]\n",
    "    - input_data: np.array [N] or None\n",
    "    - anomaly_flags: np.array(bool) [N] or None\n",
    "    - save_dir: 저장 폴더 경로\n",
    "    - interval_days: 멀티 플롯 단위 기간 (기본 10일)\n",
    "    - rain: 강수량 np.array [N] or None\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'date': tt,\n",
    "        'original': original,\n",
    "        'predicted': predicted\n",
    "    })\n",
    "    if input_data is not None:\n",
    "        df['input'] = input_data\n",
    "    if anomaly_flags is not None:\n",
    "        df['anomaly'] = anomaly_flags\n",
    "    else:\n",
    "        df['anomaly'] = False\n",
    "    if rain is not None:\n",
    "        df['rain'] = rain\n",
    "\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # 날짜 단위로 분할\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    periods = pd.date_range(start=start_date, end=end_date, freq=f'{interval_days}D')\n",
    "\n",
    "    for i in range(len(periods) - 1):\n",
    "        t0, t1 = periods[i], periods[i+1]\n",
    "        window = df[(df.index >= t0) & (df.index < t1)]\n",
    "\n",
    "        if window.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "        ax1.plot(window.index, window['original'], label='Original', color='black')\n",
    "        ax1.plot(window.index, window['predicted'], label='Reconstructed', color='green', alpha=0.7)\n",
    "        if 'input' in window.columns:\n",
    "            ax1.plot(window.index, window['input'], label='Input', color='blue', alpha=0.5)\n",
    "\n",
    "        # 이상치 시각화\n",
    "        anomaly_points = window[window['anomaly']]\n",
    "        if not anomaly_points.empty:\n",
    "            ax1.scatter(\n",
    "                anomaly_points.index,\n",
    "                anomaly_points['original'],\n",
    "                color='red',\n",
    "                label='Anomaly',\n",
    "                zorder=3\n",
    "            )\n",
    "\n",
    "        # 우측 y축: 강수량 (회색 실선)\n",
    "        if 'rain' in window.columns:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(window.index, window['rain'], label='Rainfall', color='gray', linestyle='-', alpha=0.6)\n",
    "            ax2.set_ylabel('Rainfall (mm)', color='gray')\n",
    "            ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "        ax1.set_title(f'{t0.date()} ~ {t1.date()} Sewer Level Reconstruction')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Sewer Level')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend(loc='upper left')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            fname = f\"{t0.strftime('%Y%m%d')}_{t1.strftime('%Y%m%d')}.png\"\n",
    "            plt.savefig(os.path.join(save_dir, fname), dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"✅ Saved: {fname}\")\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d5713-6c26-4a14-aabf-971cc1736303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "plot_decadal_timeseries_with_anomalies(\n",
    "    tt=tt,\n",
    "    original=original_norm_g, \n",
    "    predicted=corrected_norm_g, \n",
    "    input_data=input_seq_g,\n",
    "    anomaly_flags=anomaly_flags_g,  \n",
    "    save_dir=f'./imgs/TranAD_{seq_len}_gc/gwangjoo/',\n",
    "    interval_days=5,\n",
    "    rain = rain_value_g\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49278a-9167-41f0-9ac4-16365c3d989b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "plot_decadal_timeseries_with_anomalies(\n",
    "    tt=tt,\n",
    "    original=original_norm_c, \n",
    "    predicted=corrected_norm_c, \n",
    "    input_data=input_seq_c,\n",
    "    anomaly_flags=anomaly_flags_c,  \n",
    "    save_dir=f'./imgs/TranAD_{seq_len}_gc/changwon/',\n",
    "    interval_days=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67821b-d101-40ef-b691-8c03e8768fcb",
   "metadata": {},
   "source": [
    "## Gwangjoo anamaly statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565335a-63a8-4cf3-9319-8fdd7896d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_pointwise_anomaly_statistics(point_errors_g, anomaly_flags_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff82a2-2d57-4265-b652-11593b35622a",
   "metadata": {},
   "source": [
    "## Changwon anamaly statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b8a3f-dd55-4406-abf5-52993fb07c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summarize_pointwise_anomaly_statistics(point_errors_c, anomaly_flags_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616131-c1e4-4cf6-81bb-484369cf84bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
